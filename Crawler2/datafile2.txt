     Architecture of ZAB – ZooKeeper Atomic Broadcast protocol – Distributed Algorithm                                                                   Skip to content    Open Menu  HomeAbout    Search        Search for:     Close     Distributed Algorithm For those interested in distributed designs         Uncategorized  Architecture of ZAB – ZooKeeper Atomic Broadcast protocol  June 20, 2015August 28, 2016 Guy6 Comments    Background ZooKeeper support clients reading and updating key values pairs with high availability. High availability is achieved by replicating the data to multiple nodes and let clients read from any node. Critical to the design of Zookeeper is the observation that each state change is incremental with respect to the previous state, so there is an implicit dependence on the order of the state changes. Zookeeper Atomic Broadcast (ZAB) is the protocol under the hood that drives ZooKeeper replication order guarantee. It also handles electing a leader and recovery of failing leaders and nodes. This post is about ZAB. Definitions  leader and followers-  in ZooKeeper cluster, one of the nodes has a  leader role and the rest have followers roles. The leader is responsible for accepting all incoming state changes from the clients and replicate them to itself and to the followers. read requests are load balanced between all followers and leader. transactions –  client state changes that a leader propagates to its followers. ‘e’ – the epoch of a leader. epoch is an integer that is generated by a leader when he start to lead and should be larger than epoch’s of previous leaders. ‘c’ – a sequence number that is generated by the leader, starting at 0 and increasing. it is used together with an epoch to order the incoming clients state changes. ‘F.history’ – follower’s history queue. used for committing incoming transactions in the order they arrived. outstanding transactions – the set of transactions in the F.History that have sequence number smaller than current COMMIT sequence number.  ZAB Requirements  Replication guarantees  Reliable delivery – If a transaction, M, is committed by one server, it will be eventually committed by all servers. Total order – If transaction A is committed before transaction B by one server, A will be committed before B by all servers. If A and B are committed messages, either A will be committed before B or B will be committed before A. Causal order – If a transaction B is sent after a transaction A has been committed by the sender of B, A must be ordered before B. If a sender sends C after sending B, C must be ordered after B.   Transactions are replicated as long as majority (quorum) of nodes are up. When nodes fail and later restarted – it should catch up the transactions that were replicated during the time it was down.  ZAB Implementation  clients read from any of the ZooKeeper nodes. clients write state changes to any of the ZooKeeper nodes and this state changes are forward to the leader node. ZooKeeper uses a variation of  two-phase-commit protocol for replicating transactions to followers. When a leader receive a change update from a client it generate a transaction with sequel number c and the leader’s epoch e (see definitions) and send the transaction to all followers. a follower adds the transaction to its history queue and send ACK to the leader. When a leader receives ACK’s from a quorum it send the the quorum COMMIT for that transaction. a follower that accept COMMIT will commit this transaction unless c is higher than any sequence number in its history queue. It will wait for receiving COMMIT’s for all its earlier transactions (outstanding transactions) before commiting.   picture taken from reference [4]  Upon leader crashes, nodes execute a recovery protocol both to agree upon a common consistent state before resuming regular operation and to establish a new leader to broadcast state changes. To exercise the leader role, a node must have the support of a quorum of nodes. As nodes can crash and recover, there can be over time multiple leaders and in fact the same nodes may exercise the node role multiple times. node’s life cycle: each node executes one iteration of this protocol at a time, and at any time, a process may drop the current iteration and start a new one by proceeding to Phase 0.  Phase 0 –  prospective leader election Phase 1 – discovery Phase 2 – synchronization Phase 3 – broadcast   Phases 1 and 2 are important for bringing the ensemble to a mutually consistent state, specially when recovering from crashes. Phase 1 – Discovery In this phase, followers communicate with their prospective leader, so that the leader gathers information about the most recent transactions that its followers accepted. The purpose of this phase is to discover the most updated sequence of accepted transactions among a quorum, and to establish a new epoch so that previous leaders cannot commit new proposals. Because quorum of the followers have all changes accepted by the previous leader- then it is promised that at least one of the followers in current quorum has in its history queue all the changes accepted by previous leader which means that the new leader will have them as well. Phase 1 exact algorithm available here. Phase 2 – Synchronization The Synchronization phase concludes the recovery part of the protocol, synchronizing the replicas in the cluster using the leader’s updated history from the discovery phase. The leader communicates with the followers, proposing transactions from its history. Followers acknowledge the proposals if their own history is behind the leader’s history. When the leader sees acknowledgements from a quorum, it issues a commit message to them. At that point, the leader is said to be established, and not anymore prospective. Phase 2 exact algorithm available here. Phase 3 – Broadcast If no crashes occur, peers stay in this phase indefinitely, performing broadcast of transactions as soon as a ZooKeeper client issues a write request.  Phase 3 exact algorithm available here. To detect failures, Zab employs periodic heartbeat messages between followers and their leaders. If a leader does not receive heartbeats from a quorum of followers within a given timeout, it abandons its leadership and shifts to state election and Phase 0. A follower also goes to Leader Election Phase if it does not receive heartbeats from its leader within a timeout.  References:  Flavio P. Junqueira, Benjamin C. Reed, and Marco Serafini, “Zab: High-performance broadcast for primary-backup systems”  Andr´e Medeiros, “ZooKeeper’s atomic broadcast protocol: Theory and practice” ZooKeeper Apache documentation Benjamin Reed,Flavio P. Junqueira, “A simple totally ordered broadcast protocol”     Advertisements       Share this:TwitterFacebookGoogleLike this:Like Loading...  Related     architecture, atomic, atomic_broadcast, brodcast, commit, crash, protocol, zab, Zookeeper     Post navigation   Previous PostAlgorithm for Zab Phase 3 – BroadcastNext PostRSYNC design – a method for replicating updates      6 thoughts on “Architecture of ZAB – ZooKeeper Atomic Broadcast protocol”       Algorithm for Zab Phase 1 – Discovery | Distributed Algorithm says:     September 28, 2015 at 8:56 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Algorithm for Zab Phase 2 – Synchronization | Distributed Algorithm says:     September 28, 2015 at 8:57 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Algorithm for Zab Phase 3 – Broadcast | Distributed Algorithm says:     September 28, 2015 at 9:03 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Distributed Database Design – Notes | Tingri says:     April 16, 2016 at 9:26 pm      […] ZAB(Zookeeper Atomic Broadcast) seems to be essentially a variant of two phase commit protocol. Attached is a picture. A link explaining it further is here. […]  Reply       Decentralized Web Summit | snarfed.org says:     June 12, 2016 at 6:32 am      […] bitcoin, or send one, and those transactions are just as consistent and durable as a Paxos round or ZAB […]  Reply        www.google.co.uk says:     December 27, 2016 at 7:02 pm      Hey I am so excited I found your website, I really found you by mistake, while I was looking on Bing for something else, Regardless I am here now and would just like to say cheers for a remarkable post and a all round exciting blog (I also love the theme/design), I don’t have time to read through it all at the moment but I have bookmarked it and also added in your RSS feeds, so when I have time I will be back to read a great deal more, Please do keep up the superb job.  Reply     Leave a Reply Cancel reply     Enter your comment here...     Fill in your details below or click an icon to log in:                                    Email (required) (Address never made public)    Name (required)    Website                You are commenting using your WordPress.com account. ( Log Out / Change )              You are commenting using your Twitter account. ( Log Out / Change )              You are commenting using your Facebook account. ( Log Out / Change )              You are commenting using your Google+ account. ( Log Out / Change )     Cancel Connecting to %s      Notify me of new comments via email.                Search for:      Recent Posts   Performance test for evaluating the overhead of tracing process memory activity   The Architecture of Kafka -Distributed Message Queue   Sketchnote for One line of working code is worth 500 of specification   Sketchnote for Skyscrapers aren’t scalable   Disaster Recovery for Solr – a Story from the Field    Recent Comments  www.google.co.uk on Architecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Disaster Recovery fo… on Two-Phase-CommitDistributed Database… on Architecture of ZAB – Zo…Disaster Recovery fo… on RSYNC design – a method…  Archives  November 2016 August 2016 June 2016 February 2016 January 2016 August 2015 June 2015 May 2015 September 2014 June 2014 May 2014  Categories  file_sharing  Torrent  Uncategorized   Meta  Register Log in Entries RSS Comments RSS WordPress.com         Search for:      Recent Posts   Performance test for evaluating the overhead of tracing process memory activity   The Architecture of Kafka -Distributed Message Queue   Sketchnote for One line of working code is worth 500 of specification   Sketchnote for Skyscrapers aren’t scalable   Disaster Recovery for Solr – a Story from the Field    Recent Comments  www.google.co.uk on Architecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Disaster Recovery fo… on Two-Phase-CommitDistributed Database… on Architecture of ZAB – Zo…Disaster Recovery fo… on RSYNC design – a method…  Archives  November 2016 August 2016 June 2016 February 2016 January 2016 August 2015 June 2015 May 2015 September 2014 June 2014 May 2014  Categories  file_sharing  Torrent  Uncategorized   Meta  Register Log in Entries RSS Comments RSS WordPress.com      Blog at WordPress.com.    Back to top               Post to    Cancel                           %d bloggers like this:                Architecture of ZAB – ZooKeeper Atomic Broadcast protocol – Distributed Algorithm                                                                   Skip to content    Open Menu  HomeAbout    Search        Search for:     Close     Distributed Algorithm For those interested in distributed designs         Uncategorized  Architecture of ZAB – ZooKeeper Atomic Broadcast protocol  June 20, 2015August 28, 2016 Guy6 Comments    Background ZooKeeper support clients reading and updating key values pairs with high availability. High availability is achieved by replicating the data to multiple nodes and let clients read from any node. Critical to the design of Zookeeper is the observation that each state change is incremental with respect to the previous state, so there is an implicit dependence on the order of the state changes. Zookeeper Atomic Broadcast (ZAB) is the protocol under the hood that drives ZooKeeper replication order guarantee. It also handles electing a leader and recovery of failing leaders and nodes. This post is about ZAB. Definitions  leader and followers-  in ZooKeeper cluster, one of the nodes has a  leader role and the rest have followers roles. The leader is responsible for accepting all incoming state changes from the clients and replicate them to itself and to the followers. read requests are load balanced between all followers and leader. transactions –  client state changes that a leader propagates to its followers. ‘e’ – the epoch of a leader. epoch is an integer that is generated by a leader when he start to lead and should be larger than epoch’s of previous leaders. ‘c’ – a sequence number that is generated by the leader, starting at 0 and increasing. it is used together with an epoch to order the incoming clients state changes. ‘F.history’ – follower’s history queue. used for committing incoming transactions in the order they arrived. outstanding transactions – the set of transactions in the F.History that have sequence number smaller than current COMMIT sequence number.  ZAB Requirements  Replication guarantees  Reliable delivery – If a transaction, M, is committed by one server, it will be eventually committed by all servers. Total order – If transaction A is committed before transaction B by one server, A will be committed before B by all servers. If A and B are committed messages, either A will be committed before B or B will be committed before A. Causal order – If a transaction B is sent after a transaction A has been committed by the sender of B, A must be ordered before B. If a sender sends C after sending B, C must be ordered after B.   Transactions are replicated as long as majority (quorum) of nodes are up. When nodes fail and later restarted – it should catch up the transactions that were replicated during the time it was down.  ZAB Implementation  clients read from any of the ZooKeeper nodes. clients write state changes to any of the ZooKeeper nodes and this state changes are forward to the leader node. ZooKeeper uses a variation of  two-phase-commit protocol for replicating transactions to followers. When a leader receive a change update from a client it generate a transaction with sequel number c and the leader’s epoch e (see definitions) and send the transaction to all followers. a follower adds the transaction to its history queue and send ACK to the leader. When a leader receives ACK’s from a quorum it send the the quorum COMMIT for that transaction. a follower that accept COMMIT will commit this transaction unless c is higher than any sequence number in its history queue. It will wait for receiving COMMIT’s for all its earlier transactions (outstanding transactions) before commiting.   picture taken from reference [4]  Upon leader crashes, nodes execute a recovery protocol both to agree upon a common consistent state before resuming regular operation and to establish a new leader to broadcast state changes. To exercise the leader role, a node must have the support of a quorum of nodes. As nodes can crash and recover, there can be over time multiple leaders and in fact the same nodes may exercise the node role multiple times. node’s life cycle: each node executes one iteration of this protocol at a time, and at any time, a process may drop the current iteration and start a new one by proceeding to Phase 0.  Phase 0 –  prospective leader election Phase 1 – discovery Phase 2 – synchronization Phase 3 – broadcast   Phases 1 and 2 are important for bringing the ensemble to a mutually consistent state, specially when recovering from crashes. Phase 1 – Discovery In this phase, followers communicate with their prospective leader, so that the leader gathers information about the most recent transactions that its followers accepted. The purpose of this phase is to discover the most updated sequence of accepted transactions among a quorum, and to establish a new epoch so that previous leaders cannot commit new proposals. Because quorum of the followers have all changes accepted by the previous leader- then it is promised that at least one of the followers in current quorum has in its history queue all the changes accepted by previous leader which means that the new leader will have them as well. Phase 1 exact algorithm available here. Phase 2 – Synchronization The Synchronization phase concludes the recovery part of the protocol, synchronizing the replicas in the cluster using the leader’s updated history from the discovery phase. The leader communicates with the followers, proposing transactions from its history. Followers acknowledge the proposals if their own history is behind the leader’s history. When the leader sees acknowledgements from a quorum, it issues a commit message to them. At that point, the leader is said to be established, and not anymore prospective. Phase 2 exact algorithm available here. Phase 3 – Broadcast If no crashes occur, peers stay in this phase indefinitely, performing broadcast of transactions as soon as a ZooKeeper client issues a write request.  Phase 3 exact algorithm available here. To detect failures, Zab employs periodic heartbeat messages between followers and their leaders. If a leader does not receive heartbeats from a quorum of followers within a given timeout, it abandons its leadership and shifts to state election and Phase 0. A follower also goes to Leader Election Phase if it does not receive heartbeats from its leader within a timeout.  References:  Flavio P. Junqueira, Benjamin C. Reed, and Marco Serafini, “Zab: High-performance broadcast for primary-backup systems”  Andr´e Medeiros, “ZooKeeper’s atomic broadcast protocol: Theory and practice” ZooKeeper Apache documentation Benjamin Reed,Flavio P. Junqueira, “A simple totally ordered broadcast protocol”     Advertisements       Share this:TwitterFacebookGoogleLike this:Like Loading...  Related     architecture, atomic, atomic_broadcast, brodcast, commit, crash, protocol, zab, Zookeeper     Post navigation   Previous PostAlgorithm for Zab Phase 3 – BroadcastNext PostRSYNC design – a method for replicating updates      6 thoughts on “Architecture of ZAB – ZooKeeper Atomic Broadcast protocol”       Algorithm for Zab Phase 1 – Discovery | Distributed Algorithm says:     September 28, 2015 at 8:56 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Algorithm for Zab Phase 2 – Synchronization | Distributed Algorithm says:     September 28, 2015 at 8:57 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Algorithm for Zab Phase 3 – Broadcast | Distributed Algorithm says:     September 28, 2015 at 9:03 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Distributed Database Design – Notes | Tingri says:     April 16, 2016 at 9:26 pm      […] ZAB(Zookeeper Atomic Broadcast) seems to be essentially a variant of two phase commit protocol. Attached is a picture. A link explaining it further is here. […]  Reply       Decentralized Web Summit | snarfed.org says:     June 12, 2016 at 6:32 am      […] bitcoin, or send one, and those transactions are just as consistent and durable as a Paxos round or ZAB […]  Reply        www.google.co.uk says:     December 27, 2016 at 7:02 pm      Hey I am so excited I found your website, I really found you by mistake, while I was looking on Bing for something else, Regardless I am here now and would just like to say cheers for a remarkable post and a all round exciting blog (I also love the theme/design), I don’t have time to read through it all at the moment but I have bookmarked it and also added in your RSS feeds, so when I have time I will be back to read a great deal more, Please do keep up the superb job.  Reply     Leave a Reply Cancel reply     Enter your comment here...     Fill in your details below or click an icon to log in:                                    Email (required) (Address never made public)    Name (required)    Website                You are commenting using your WordPress.com account. ( Log Out / Change )              You are commenting using your Twitter account. ( Log Out / Change )              You are commenting using your Facebook account. ( Log Out / Change )              You are commenting using your Google+ account. ( Log Out / Change )     Cancel Connecting to %s      Notify me of new comments via email.                Search for:      Recent Posts   Performance test for evaluating the overhead of tracing process memory activity   The Architecture of Kafka -Distributed Message Queue   Sketchnote for One line of working code is worth 500 of specification   Sketchnote for Skyscrapers aren’t scalable   Disaster Recovery for Solr – a Story from the Field    Recent Comments  www.google.co.uk on Architecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Disaster Recovery fo… on Two-Phase-CommitDistributed Database… on Architecture of ZAB – Zo…Disaster Recovery fo… on RSYNC design – a method…  Archives  November 2016 August 2016 June 2016 February 2016 January 2016 August 2015 June 2015 May 2015 September 2014 June 2014 May 2014  Categories  file_sharing  Torrent  Uncategorized   Meta  Register Log in Entries RSS Comments RSS WordPress.com         Search for:      Recent Posts   Performance test for evaluating the overhead of tracing process memory activity   The Architecture of Kafka -Distributed Message Queue   Sketchnote for One line of working code is worth 500 of specification   Sketchnote for Skyscrapers aren’t scalable   Disaster Recovery for Solr – a Story from the Field    Recent Comments  www.google.co.uk on Architecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Disaster Recovery fo… on Two-Phase-CommitDistributed Database… on Architecture of ZAB – Zo…Disaster Recovery fo… on RSYNC design – a method…  Archives  November 2016 August 2016 June 2016 February 2016 January 2016 August 2015 June 2015 May 2015 September 2014 June 2014 May 2014  Categories  file_sharing  Torrent  Uncategorized   Meta  Register Log in Entries RSS Comments RSS WordPress.com      Blog at WordPress.com.    Back to top               Post to    Cancel                           %d bloggers like this:                Architecture of ZAB – ZooKeeper Atomic Broadcast protocol – Distributed Algorithm                                                                Architecture of ZAB – ZooKeeper Atomic Broadcast protocol – Distributed Algorithm  Skip to content    Open Menu  HomeAbout    Search        Search for:     Close     Distributed Algorithm For those interested in distributed designs         Uncategorized  Architecture of ZAB – ZooKeeper Atomic Broadcast protocol  June 20, 2015August 28, 2016 Guy6 Comments    Background ZooKeeper support clients reading and updating key values pairs with high availability. High availability is achieved by replicating the data to multiple nodes and let clients read from any node. Critical to the design of Zookeeper is the observation that each state change is incremental with respect to the previous state, so there is an implicit dependence on the order of the state changes. Zookeeper Atomic Broadcast (ZAB) is the protocol under the hood that drives ZooKeeper replication order guarantee. It also handles electing a leader and recovery of failing leaders and nodes. This post is about ZAB. Definitions  leader and followers-  in ZooKeeper cluster, one of the nodes has a  leader role and the rest have followers roles. The leader is responsible for accepting all incoming state changes from the clients and replicate them to itself and to the followers. read requests are load balanced between all followers and leader. transactions –  client state changes that a leader propagates to its followers. ‘e’ – the epoch of a leader. epoch is an integer that is generated by a leader when he start to lead and should be larger than epoch’s of previous leaders. ‘c’ – a sequence number that is generated by the leader, starting at 0 and increasing. it is used together with an epoch to order the incoming clients state changes. ‘F.history’ – follower’s history queue. used for committing incoming transactions in the order they arrived. outstanding transactions – the set of transactions in the F.History that have sequence number smaller than current COMMIT sequence number.  ZAB Requirements  Replication guarantees  Reliable delivery – If a transaction, M, is committed by one server, it will be eventually committed by all servers. Total order – If transaction A is committed before transaction B by one server, A will be committed before B by all servers. If A and B are committed messages, either A will be committed before B or B will be committed before A. Causal order – If a transaction B is sent after a transaction A has been committed by the sender of B, A must be ordered before B. If a sender sends C after sending B, C must be ordered after B.   Transactions are replicated as long as majority (quorum) of nodes are up. When nodes fail and later restarted – it should catch up the transactions that were replicated during the time it was down.  ZAB Implementation  clients read from any of the ZooKeeper nodes. clients write state changes to any of the ZooKeeper nodes and this state changes are forward to the leader node. ZooKeeper uses a variation of  two-phase-commit protocol for replicating transactions to followers. When a leader receive a change update from a client it generate a transaction with sequel number c and the leader’s epoch e (see definitions) and send the transaction to all followers. a follower adds the transaction to its history queue and send ACK to the leader. When a leader receives ACK’s from a quorum it send the the quorum COMMIT for that transaction. a follower that accept COMMIT will commit this transaction unless c is higher than any sequence number in its history queue. It will wait for receiving COMMIT’s for all its earlier transactions (outstanding transactions) before commiting.   picture taken from reference [4]  Upon leader crashes, nodes execute a recovery protocol both to agree upon a common consistent state before resuming regular operation and to establish a new leader to broadcast state changes. To exercise the leader role, a node must have the support of a quorum of nodes. As nodes can crash and recover, there can be over time multiple leaders and in fact the same nodes may exercise the node role multiple times. node’s life cycle: each node executes one iteration of this protocol at a time, and at any time, a process may drop the current iteration and start a new one by proceeding to Phase 0.  Phase 0 –  prospective leader election Phase 1 – discovery Phase 2 – synchronization Phase 3 – broadcast   Phases 1 and 2 are important for bringing the ensemble to a mutually consistent state, specially when recovering from crashes. Phase 1 – Discovery In this phase, followers communicate with their prospective leader, so that the leader gathers information about the most recent transactions that its followers accepted. The purpose of this phase is to discover the most updated sequence of accepted transactions among a quorum, and to establish a new epoch so that previous leaders cannot commit new proposals. Because quorum of the followers have all changes accepted by the previous leader- then it is promised that at least one of the followers in current quorum has in its history queue all the changes accepted by previous leader which means that the new leader will have them as well. Phase 1 exact algorithm available here. Phase 2 – Synchronization The Synchronization phase concludes the recovery part of the protocol, synchronizing the replicas in the cluster using the leader’s updated history from the discovery phase. The leader communicates with the followers, proposing transactions from its history. Followers acknowledge the proposals if their own history is behind the leader’s history. When the leader sees acknowledgements from a quorum, it issues a commit message to them. At that point, the leader is said to be established, and not anymore prospective. Phase 2 exact algorithm available here. Phase 3 – Broadcast If no crashes occur, peers stay in this phase indefinitely, performing broadcast of transactions as soon as a ZooKeeper client issues a write request.  Phase 3 exact algorithm available here. To detect failures, Zab employs periodic heartbeat messages between followers and their leaders. If a leader does not receive heartbeats from a quorum of followers within a given timeout, it abandons its leadership and shifts to state election and Phase 0. A follower also goes to Leader Election Phase if it does not receive heartbeats from its leader within a timeout.  References:  Flavio P. Junqueira, Benjamin C. Reed, and Marco Serafini, “Zab: High-performance broadcast for primary-backup systems”  Andr´e Medeiros, “ZooKeeper’s atomic broadcast protocol: Theory and practice” ZooKeeper Apache documentation Benjamin Reed,Flavio P. Junqueira, “A simple totally ordered broadcast protocol”     Advertisements       Share this:TwitterFacebookGoogleLike this:Like Loading...  Related     architecture, atomic, atomic_broadcast, brodcast, commit, crash, protocol, zab, Zookeeper     Post navigation   Previous PostAlgorithm for Zab Phase 3 – BroadcastNext PostRSYNC design – a method for replicating updates      6 thoughts on “Architecture of ZAB – ZooKeeper Atomic Broadcast protocol”       Algorithm for Zab Phase 1 – Discovery | Distributed Algorithm says:     September 28, 2015 at 8:56 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Algorithm for Zab Phase 2 – Synchronization | Distributed Algorithm says:     September 28, 2015 at 8:57 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Algorithm for Zab Phase 3 – Broadcast | Distributed Algorithm says:     September 28, 2015 at 9:03 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Distributed Database Design – Notes | Tingri says:     April 16, 2016 at 9:26 pm      […] ZAB(Zookeeper Atomic Broadcast) seems to be essentially a variant of two phase commit protocol. Attached is a picture. A link explaining it further is here. […]  Reply       Decentralized Web Summit | snarfed.org says:     June 12, 2016 at 6:32 am      […] bitcoin, or send one, and those transactions are just as consistent and durable as a Paxos round or ZAB […]  Reply        www.google.co.uk says:     December 27, 2016 at 7:02 pm      Hey I am so excited I found your website, I really found you by mistake, while I was looking on Bing for something else, Regardless I am here now and would just like to say cheers for a remarkable post and a all round exciting blog (I also love the theme/design), I don’t have time to read through it all at the moment but I have bookmarked it and also added in your RSS feeds, so when I have time I will be back to read a great deal more, Please do keep up the superb job.  Reply     Leave a Reply Cancel reply     Enter your comment here...     Fill in your details below or click an icon to log in:                                    Email (required) (Address never made public)    Name (required)    Website                You are commenting using your WordPress.com account. ( Log Out / Change )              You are commenting using your Twitter account. ( Log Out / Change )              You are commenting using your Facebook account. ( Log Out / Change )              You are commenting using your Google+ account. ( Log Out / Change )     Cancel Connecting to %s      Notify me of new comments via email.                Search for:      Recent Posts   Performance test for evaluating the overhead of tracing process memory activity   The Architecture of Kafka -Distributed Message Queue   Sketchnote for One line of working code is worth 500 of specification   Sketchnote for Skyscrapers aren’t scalable   Disaster Recovery for Solr – a Story from the Field    Recent Comments  www.google.co.uk on Architecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Disaster Recovery fo… on Two-Phase-CommitDistributed Database… on Architecture of ZAB – Zo…Disaster Recovery fo… on RSYNC design – a method…  Archives  November 2016 August 2016 June 2016 February 2016 January 2016 August 2015 June 2015 May 2015 September 2014 June 2014 May 2014  Categories  file_sharing  Torrent  Uncategorized   Meta  Register Log in Entries RSS Comments RSS WordPress.com         Search for:      Recent Posts   Performance test for evaluating the overhead of tracing process memory activity   The Architecture of Kafka -Distributed Message Queue   Sketchnote for One line of working code is worth 500 of specification   Sketchnote for Skyscrapers aren’t scalable   Disaster Recovery for Solr – a Story from the Field    Recent Comments  www.google.co.uk on Architecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Disaster Recovery fo… on Two-Phase-CommitDistributed Database… on Architecture of ZAB – Zo…Disaster Recovery fo… on RSYNC design – a method…  Archives  November 2016 August 2016 June 2016 February 2016 January 2016 August 2015 June 2015 May 2015 September 2014 June 2014 May 2014  Categories  file_sharing  Torrent  Uncategorized   Meta  Register Log in Entries RSS Comments RSS WordPress.com      Blog at WordPress.com.    Back to top               Post to    Cancel                           %d bloggers like this:            Skip to content    Open Menu  HomeAbout    Search        Search for:     Close     Distributed Algorithm For those interested in distributed designs         Uncategorized  Architecture of ZAB – ZooKeeper Atomic Broadcast protocol  June 20, 2015August 28, 2016 Guy6 Comments    Background ZooKeeper support clients reading and updating key values pairs with high availability. High availability is achieved by replicating the data to multiple nodes and let clients read from any node. Critical to the design of Zookeeper is the observation that each state change is incremental with respect to the previous state, so there is an implicit dependence on the order of the state changes. Zookeeper Atomic Broadcast (ZAB) is the protocol under the hood that drives ZooKeeper replication order guarantee. It also handles electing a leader and recovery of failing leaders and nodes. This post is about ZAB. Definitions  leader and followers-  in ZooKeeper cluster, one of the nodes has a  leader role and the rest have followers roles. The leader is responsible for accepting all incoming state changes from the clients and replicate them to itself and to the followers. read requests are load balanced between all followers and leader. transactions –  client state changes that a leader propagates to its followers. ‘e’ – the epoch of a leader. epoch is an integer that is generated by a leader when he start to lead and should be larger than epoch’s of previous leaders. ‘c’ – a sequence number that is generated by the leader, starting at 0 and increasing. it is used together with an epoch to order the incoming clients state changes. ‘F.history’ – follower’s history queue. used for committing incoming transactions in the order they arrived. outstanding transactions – the set of transactions in the F.History that have sequence number smaller than current COMMIT sequence number.  ZAB Requirements  Replication guarantees  Reliable delivery – If a transaction, M, is committed by one server, it will be eventually committed by all servers. Total order – If transaction A is committed before transaction B by one server, A will be committed before B by all servers. If A and B are committed messages, either A will be committed before B or B will be committed before A. Causal order – If a transaction B is sent after a transaction A has been committed by the sender of B, A must be ordered before B. If a sender sends C after sending B, C must be ordered after B.   Transactions are replicated as long as majority (quorum) of nodes are up. When nodes fail and later restarted – it should catch up the transactions that were replicated during the time it was down.  ZAB Implementation  clients read from any of the ZooKeeper nodes. clients write state changes to any of the ZooKeeper nodes and this state changes are forward to the leader node. ZooKeeper uses a variation of  two-phase-commit protocol for replicating transactions to followers. When a leader receive a change update from a client it generate a transaction with sequel number c and the leader’s epoch e (see definitions) and send the transaction to all followers. a follower adds the transaction to its history queue and send ACK to the leader. When a leader receives ACK’s from a quorum it send the the quorum COMMIT for that transaction. a follower that accept COMMIT will commit this transaction unless c is higher than any sequence number in its history queue. It will wait for receiving COMMIT’s for all its earlier transactions (outstanding transactions) before commiting.   picture taken from reference [4]  Upon leader crashes, nodes execute a recovery protocol both to agree upon a common consistent state before resuming regular operation and to establish a new leader to broadcast state changes. To exercise the leader role, a node must have the support of a quorum of nodes. As nodes can crash and recover, there can be over time multiple leaders and in fact the same nodes may exercise the node role multiple times. node’s life cycle: each node executes one iteration of this protocol at a time, and at any time, a process may drop the current iteration and start a new one by proceeding to Phase 0.  Phase 0 –  prospective leader election Phase 1 – discovery Phase 2 – synchronization Phase 3 – broadcast   Phases 1 and 2 are important for bringing the ensemble to a mutually consistent state, specially when recovering from crashes. Phase 1 – Discovery In this phase, followers communicate with their prospective leader, so that the leader gathers information about the most recent transactions that its followers accepted. The purpose of this phase is to discover the most updated sequence of accepted transactions among a quorum, and to establish a new epoch so that previous leaders cannot commit new proposals. Because quorum of the followers have all changes accepted by the previous leader- then it is promised that at least one of the followers in current quorum has in its history queue all the changes accepted by previous leader which means that the new leader will have them as well. Phase 1 exact algorithm available here. Phase 2 – Synchronization The Synchronization phase concludes the recovery part of the protocol, synchronizing the replicas in the cluster using the leader’s updated history from the discovery phase. The leader communicates with the followers, proposing transactions from its history. Followers acknowledge the proposals if their own history is behind the leader’s history. When the leader sees acknowledgements from a quorum, it issues a commit message to them. At that point, the leader is said to be established, and not anymore prospective. Phase 2 exact algorithm available here. Phase 3 – Broadcast If no crashes occur, peers stay in this phase indefinitely, performing broadcast of transactions as soon as a ZooKeeper client issues a write request.  Phase 3 exact algorithm available here. To detect failures, Zab employs periodic heartbeat messages between followers and their leaders. If a leader does not receive heartbeats from a quorum of followers within a given timeout, it abandons its leadership and shifts to state election and Phase 0. A follower also goes to Leader Election Phase if it does not receive heartbeats from its leader within a timeout.  References:  Flavio P. Junqueira, Benjamin C. Reed, and Marco Serafini, “Zab: High-performance broadcast for primary-backup systems”  Andr´e Medeiros, “ZooKeeper’s atomic broadcast protocol: Theory and practice” ZooKeeper Apache documentation Benjamin Reed,Flavio P. Junqueira, “A simple totally ordered broadcast protocol”     Advertisements       Share this:TwitterFacebookGoogleLike this:Like Loading...  Related     architecture, atomic, atomic_broadcast, brodcast, commit, crash, protocol, zab, Zookeeper     Post navigation   Previous PostAlgorithm for Zab Phase 3 – BroadcastNext PostRSYNC design – a method for replicating updates      6 thoughts on “Architecture of ZAB – ZooKeeper Atomic Broadcast protocol”       Algorithm for Zab Phase 1 – Discovery | Distributed Algorithm says:     September 28, 2015 at 8:56 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Algorithm for Zab Phase 2 – Synchronization | Distributed Algorithm says:     September 28, 2015 at 8:57 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Algorithm for Zab Phase 3 – Broadcast | Distributed Algorithm says:     September 28, 2015 at 9:03 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Distributed Database Design – Notes | Tingri says:     April 16, 2016 at 9:26 pm      […] ZAB(Zookeeper Atomic Broadcast) seems to be essentially a variant of two phase commit protocol. Attached is a picture. A link explaining it further is here. […]  Reply       Decentralized Web Summit | snarfed.org says:     June 12, 2016 at 6:32 am      […] bitcoin, or send one, and those transactions are just as consistent and durable as a Paxos round or ZAB […]  Reply        www.google.co.uk says:     December 27, 2016 at 7:02 pm      Hey I am so excited I found your website, I really found you by mistake, while I was looking on Bing for something else, Regardless I am here now and would just like to say cheers for a remarkable post and a all round exciting blog (I also love the theme/design), I don’t have time to read through it all at the moment but I have bookmarked it and also added in your RSS feeds, so when I have time I will be back to read a great deal more, Please do keep up the superb job.  Reply     Leave a Reply Cancel reply     Enter your comment here...     Fill in your details below or click an icon to log in:                                    Email (required) (Address never made public)    Name (required)    Website                You are commenting using your WordPress.com account. ( Log Out / Change )              You are commenting using your Twitter account. ( Log Out / Change )              You are commenting using your Facebook account. ( Log Out / Change )              You are commenting using your Google+ account. ( Log Out / Change )     Cancel Connecting to %s      Notify me of new comments via email.                Search for:      Recent Posts   Performance test for evaluating the overhead of tracing process memory activity   The Architecture of Kafka -Distributed Message Queue   Sketchnote for One line of working code is worth 500 of specification   Sketchnote for Skyscrapers aren’t scalable   Disaster Recovery for Solr – a Story from the Field    Recent Comments  www.google.co.uk on Architecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Disaster Recovery fo… on Two-Phase-CommitDistributed Database… on Architecture of ZAB – Zo…Disaster Recovery fo… on RSYNC design – a method…  Archives  November 2016 August 2016 June 2016 February 2016 January 2016 August 2015 June 2015 May 2015 September 2014 June 2014 May 2014  Categories  file_sharing  Torrent  Uncategorized   Meta  Register Log in Entries RSS Comments RSS WordPress.com         Search for:      Recent Posts   Performance test for evaluating the overhead of tracing process memory activity   The Architecture of Kafka -Distributed Message Queue   Sketchnote for One line of working code is worth 500 of specification   Sketchnote for Skyscrapers aren’t scalable   Disaster Recovery for Solr – a Story from the Field    Recent Comments  www.google.co.uk on Architecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Disaster Recovery fo… on Two-Phase-CommitDistributed Database… on Architecture of ZAB – Zo…Disaster Recovery fo… on RSYNC design – a method…  Archives  November 2016 August 2016 June 2016 February 2016 January 2016 August 2015 June 2015 May 2015 September 2014 June 2014 May 2014  Categories  file_sharing  Torrent  Uncategorized   Meta  Register Log in Entries RSS Comments RSS WordPress.com      Blog at WordPress.com.   Skip to content   Open Menu  HomeAbout    Search     Open Menu  HomeAbout    Search   Open Menu  HomeAbout  Open MenuOpen Menu HomeAbout  HomeAbout HomeHomeAboutAbout Search SearchSearchSearch    Search for:     Close     Search for:     Close   Search for:     Search for:  Search for:CloseClose  Distributed Algorithm For those interested in distributed designs   Distributed Algorithm For those interested in distributed designs Distributed AlgorithmDistributed AlgorithmFor those interested in distributed designs      Uncategorized  Architecture of ZAB – ZooKeeper Atomic Broadcast protocol  June 20, 2015August 28, 2016 Guy6 Comments    Background ZooKeeper support clients reading and updating key values pairs with high availability. High availability is achieved by replicating the data to multiple nodes and let clients read from any node. Critical to the design of Zookeeper is the observation that each state change is incremental with respect to the previous state, so there is an implicit dependence on the order of the state changes. Zookeeper Atomic Broadcast (ZAB) is the protocol under the hood that drives ZooKeeper replication order guarantee. It also handles electing a leader and recovery of failing leaders and nodes. This post is about ZAB. Definitions  leader and followers-  in ZooKeeper cluster, one of the nodes has a  leader role and the rest have followers roles. The leader is responsible for accepting all incoming state changes from the clients and replicate them to itself and to the followers. read requests are load balanced between all followers and leader. transactions –  client state changes that a leader propagates to its followers. ‘e’ – the epoch of a leader. epoch is an integer that is generated by a leader when he start to lead and should be larger than epoch’s of previous leaders. ‘c’ – a sequence number that is generated by the leader, starting at 0 and increasing. it is used together with an epoch to order the incoming clients state changes. ‘F.history’ – follower’s history queue. used for committing incoming transactions in the order they arrived. outstanding transactions – the set of transactions in the F.History that have sequence number smaller than current COMMIT sequence number.  ZAB Requirements  Replication guarantees  Reliable delivery – If a transaction, M, is committed by one server, it will be eventually committed by all servers. Total order – If transaction A is committed before transaction B by one server, A will be committed before B by all servers. If A and B are committed messages, either A will be committed before B or B will be committed before A. Causal order – If a transaction B is sent after a transaction A has been committed by the sender of B, A must be ordered before B. If a sender sends C after sending B, C must be ordered after B.   Transactions are replicated as long as majority (quorum) of nodes are up. When nodes fail and later restarted – it should catch up the transactions that were replicated during the time it was down.  ZAB Implementation  clients read from any of the ZooKeeper nodes. clients write state changes to any of the ZooKeeper nodes and this state changes are forward to the leader node. ZooKeeper uses a variation of  two-phase-commit protocol for replicating transactions to followers. When a leader receive a change update from a client it generate a transaction with sequel number c and the leader’s epoch e (see definitions) and send the transaction to all followers. a follower adds the transaction to its history queue and send ACK to the leader. When a leader receives ACK’s from a quorum it send the the quorum COMMIT for that transaction. a follower that accept COMMIT will commit this transaction unless c is higher than any sequence number in its history queue. It will wait for receiving COMMIT’s for all its earlier transactions (outstanding transactions) before commiting.   picture taken from reference [4]  Upon leader crashes, nodes execute a recovery protocol both to agree upon a common consistent state before resuming regular operation and to establish a new leader to broadcast state changes. To exercise the leader role, a node must have the support of a quorum of nodes. As nodes can crash and recover, there can be over time multiple leaders and in fact the same nodes may exercise the node role multiple times. node’s life cycle: each node executes one iteration of this protocol at a time, and at any time, a process may drop the current iteration and start a new one by proceeding to Phase 0.  Phase 0 –  prospective leader election Phase 1 – discovery Phase 2 – synchronization Phase 3 – broadcast   Phases 1 and 2 are important for bringing the ensemble to a mutually consistent state, specially when recovering from crashes. Phase 1 – Discovery In this phase, followers communicate with their prospective leader, so that the leader gathers information about the most recent transactions that its followers accepted. The purpose of this phase is to discover the most updated sequence of accepted transactions among a quorum, and to establish a new epoch so that previous leaders cannot commit new proposals. Because quorum of the followers have all changes accepted by the previous leader- then it is promised that at least one of the followers in current quorum has in its history queue all the changes accepted by previous leader which means that the new leader will have them as well. Phase 1 exact algorithm available here. Phase 2 – Synchronization The Synchronization phase concludes the recovery part of the protocol, synchronizing the replicas in the cluster using the leader’s updated history from the discovery phase. The leader communicates with the followers, proposing transactions from its history. Followers acknowledge the proposals if their own history is behind the leader’s history. When the leader sees acknowledgements from a quorum, it issues a commit message to them. At that point, the leader is said to be established, and not anymore prospective. Phase 2 exact algorithm available here. Phase 3 – Broadcast If no crashes occur, peers stay in this phase indefinitely, performing broadcast of transactions as soon as a ZooKeeper client issues a write request.  Phase 3 exact algorithm available here. To detect failures, Zab employs periodic heartbeat messages between followers and their leaders. If a leader does not receive heartbeats from a quorum of followers within a given timeout, it abandons its leadership and shifts to state election and Phase 0. A follower also goes to Leader Election Phase if it does not receive heartbeats from its leader within a timeout.  References:  Flavio P. Junqueira, Benjamin C. Reed, and Marco Serafini, “Zab: High-performance broadcast for primary-backup systems”  Andr´e Medeiros, “ZooKeeper’s atomic broadcast protocol: Theory and practice” ZooKeeper Apache documentation Benjamin Reed,Flavio P. Junqueira, “A simple totally ordered broadcast protocol”     Advertisements       Share this:TwitterFacebookGoogleLike this:Like Loading...  Related     architecture, atomic, atomic_broadcast, brodcast, commit, crash, protocol, zab, Zookeeper     Post navigation   Previous PostAlgorithm for Zab Phase 3 – BroadcastNext PostRSYNC design – a method for replicating updates      6 thoughts on “Architecture of ZAB – ZooKeeper Atomic Broadcast protocol”       Algorithm for Zab Phase 1 – Discovery | Distributed Algorithm says:     September 28, 2015 at 8:56 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Algorithm for Zab Phase 2 – Synchronization | Distributed Algorithm says:     September 28, 2015 at 8:57 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Algorithm for Zab Phase 3 – Broadcast | Distributed Algorithm says:     September 28, 2015 at 9:03 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Distributed Database Design – Notes | Tingri says:     April 16, 2016 at 9:26 pm      […] ZAB(Zookeeper Atomic Broadcast) seems to be essentially a variant of two phase commit protocol. Attached is a picture. A link explaining it further is here. […]  Reply       Decentralized Web Summit | snarfed.org says:     June 12, 2016 at 6:32 am      […] bitcoin, or send one, and those transactions are just as consistent and durable as a Paxos round or ZAB […]  Reply        www.google.co.uk says:     December 27, 2016 at 7:02 pm      Hey I am so excited I found your website, I really found you by mistake, while I was looking on Bing for something else, Regardless I am here now and would just like to say cheers for a remarkable post and a all round exciting blog (I also love the theme/design), I don’t have time to read through it all at the moment but I have bookmarked it and also added in your RSS feeds, so when I have time I will be back to read a great deal more, Please do keep up the superb job.  Reply     Leave a Reply Cancel reply     Enter your comment here...     Fill in your details below or click an icon to log in:                                    Email (required) (Address never made public)    Name (required)    Website                You are commenting using your WordPress.com account. ( Log Out / Change )              You are commenting using your Twitter account. ( Log Out / Change )              You are commenting using your Facebook account. ( Log Out / Change )              You are commenting using your Google+ account. ( Log Out / Change )     Cancel Connecting to %s      Notify me of new comments via email.                Search for:      Recent Posts   Performance test for evaluating the overhead of tracing process memory activity   The Architecture of Kafka -Distributed Message Queue   Sketchnote for One line of working code is worth 500 of specification   Sketchnote for Skyscrapers aren’t scalable   Disaster Recovery for Solr – a Story from the Field    Recent Comments  www.google.co.uk on Architecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Disaster Recovery fo… on Two-Phase-CommitDistributed Database… on Architecture of ZAB – Zo…Disaster Recovery fo… on RSYNC design – a method…  Archives  November 2016 August 2016 June 2016 February 2016 January 2016 August 2015 June 2015 May 2015 September 2014 June 2014 May 2014  Categories  file_sharing  Torrent  Uncategorized   Meta  Register Log in Entries RSS Comments RSS WordPress.com        Uncategorized  Architecture of ZAB – ZooKeeper Atomic Broadcast protocol  June 20, 2015August 28, 2016 Guy6 Comments    Background ZooKeeper support clients reading and updating key values pairs with high availability. High availability is achieved by replicating the data to multiple nodes and let clients read from any node. Critical to the design of Zookeeper is the observation that each state change is incremental with respect to the previous state, so there is an implicit dependence on the order of the state changes. Zookeeper Atomic Broadcast (ZAB) is the protocol under the hood that drives ZooKeeper replication order guarantee. It also handles electing a leader and recovery of failing leaders and nodes. This post is about ZAB. Definitions  leader and followers-  in ZooKeeper cluster, one of the nodes has a  leader role and the rest have followers roles. The leader is responsible for accepting all incoming state changes from the clients and replicate them to itself and to the followers. read requests are load balanced between all followers and leader. transactions –  client state changes that a leader propagates to its followers. ‘e’ – the epoch of a leader. epoch is an integer that is generated by a leader when he start to lead and should be larger than epoch’s of previous leaders. ‘c’ – a sequence number that is generated by the leader, starting at 0 and increasing. it is used together with an epoch to order the incoming clients state changes. ‘F.history’ – follower’s history queue. used for committing incoming transactions in the order they arrived. outstanding transactions – the set of transactions in the F.History that have sequence number smaller than current COMMIT sequence number.  ZAB Requirements  Replication guarantees  Reliable delivery – If a transaction, M, is committed by one server, it will be eventually committed by all servers. Total order – If transaction A is committed before transaction B by one server, A will be committed before B by all servers. If A and B are committed messages, either A will be committed before B or B will be committed before A. Causal order – If a transaction B is sent after a transaction A has been committed by the sender of B, A must be ordered before B. If a sender sends C after sending B, C must be ordered after B.   Transactions are replicated as long as majority (quorum) of nodes are up. When nodes fail and later restarted – it should catch up the transactions that were replicated during the time it was down.  ZAB Implementation  clients read from any of the ZooKeeper nodes. clients write state changes to any of the ZooKeeper nodes and this state changes are forward to the leader node. ZooKeeper uses a variation of  two-phase-commit protocol for replicating transactions to followers. When a leader receive a change update from a client it generate a transaction with sequel number c and the leader’s epoch e (see definitions) and send the transaction to all followers. a follower adds the transaction to its history queue and send ACK to the leader. When a leader receives ACK’s from a quorum it send the the quorum COMMIT for that transaction. a follower that accept COMMIT will commit this transaction unless c is higher than any sequence number in its history queue. It will wait for receiving COMMIT’s for all its earlier transactions (outstanding transactions) before commiting.   picture taken from reference [4]  Upon leader crashes, nodes execute a recovery protocol both to agree upon a common consistent state before resuming regular operation and to establish a new leader to broadcast state changes. To exercise the leader role, a node must have the support of a quorum of nodes. As nodes can crash and recover, there can be over time multiple leaders and in fact the same nodes may exercise the node role multiple times. node’s life cycle: each node executes one iteration of this protocol at a time, and at any time, a process may drop the current iteration and start a new one by proceeding to Phase 0.  Phase 0 –  prospective leader election Phase 1 – discovery Phase 2 – synchronization Phase 3 – broadcast   Phases 1 and 2 are important for bringing the ensemble to a mutually consistent state, specially when recovering from crashes. Phase 1 – Discovery In this phase, followers communicate with their prospective leader, so that the leader gathers information about the most recent transactions that its followers accepted. The purpose of this phase is to discover the most updated sequence of accepted transactions among a quorum, and to establish a new epoch so that previous leaders cannot commit new proposals. Because quorum of the followers have all changes accepted by the previous leader- then it is promised that at least one of the followers in current quorum has in its history queue all the changes accepted by previous leader which means that the new leader will have them as well. Phase 1 exact algorithm available here. Phase 2 – Synchronization The Synchronization phase concludes the recovery part of the protocol, synchronizing the replicas in the cluster using the leader’s updated history from the discovery phase. The leader communicates with the followers, proposing transactions from its history. Followers acknowledge the proposals if their own history is behind the leader’s history. When the leader sees acknowledgements from a quorum, it issues a commit message to them. At that point, the leader is said to be established, and not anymore prospective. Phase 2 exact algorithm available here. Phase 3 – Broadcast If no crashes occur, peers stay in this phase indefinitely, performing broadcast of transactions as soon as a ZooKeeper client issues a write request.  Phase 3 exact algorithm available here. To detect failures, Zab employs periodic heartbeat messages between followers and their leaders. If a leader does not receive heartbeats from a quorum of followers within a given timeout, it abandons its leadership and shifts to state election and Phase 0. A follower also goes to Leader Election Phase if it does not receive heartbeats from its leader within a timeout.  References:  Flavio P. Junqueira, Benjamin C. Reed, and Marco Serafini, “Zab: High-performance broadcast for primary-backup systems”  Andr´e Medeiros, “ZooKeeper’s atomic broadcast protocol: Theory and practice” ZooKeeper Apache documentation Benjamin Reed,Flavio P. Junqueira, “A simple totally ordered broadcast protocol”     Advertisements       Share this:TwitterFacebookGoogleLike this:Like Loading...  Related     architecture, atomic, atomic_broadcast, brodcast, commit, crash, protocol, zab, Zookeeper     Post navigation   Previous PostAlgorithm for Zab Phase 3 – BroadcastNext PostRSYNC design – a method for replicating updates      6 thoughts on “Architecture of ZAB – ZooKeeper Atomic Broadcast protocol”       Algorithm for Zab Phase 1 – Discovery | Distributed Algorithm says:     September 28, 2015 at 8:56 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Algorithm for Zab Phase 2 – Synchronization | Distributed Algorithm says:     September 28, 2015 at 8:57 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Algorithm for Zab Phase 3 – Broadcast | Distributed Algorithm says:     September 28, 2015 at 9:03 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Distributed Database Design – Notes | Tingri says:     April 16, 2016 at 9:26 pm      […] ZAB(Zookeeper Atomic Broadcast) seems to be essentially a variant of two phase commit protocol. Attached is a picture. A link explaining it further is here. […]  Reply       Decentralized Web Summit | snarfed.org says:     June 12, 2016 at 6:32 am      […] bitcoin, or send one, and those transactions are just as consistent and durable as a Paxos round or ZAB […]  Reply        www.google.co.uk says:     December 27, 2016 at 7:02 pm      Hey I am so excited I found your website, I really found you by mistake, while I was looking on Bing for something else, Regardless I am here now and would just like to say cheers for a remarkable post and a all round exciting blog (I also love the theme/design), I don’t have time to read through it all at the moment but I have bookmarked it and also added in your RSS feeds, so when I have time I will be back to read a great deal more, Please do keep up the superb job.  Reply     Leave a Reply Cancel reply     Enter your comment here...     Fill in your details below or click an icon to log in:                                    Email (required) (Address never made public)    Name (required)    Website                You are commenting using your WordPress.com account. ( Log Out / Change )              You are commenting using your Twitter account. ( Log Out / Change )              You are commenting using your Facebook account. ( Log Out / Change )              You are commenting using your Google+ account. ( Log Out / Change )     Cancel Connecting to %s      Notify me of new comments via email.                Uncategorized  Architecture of ZAB – ZooKeeper Atomic Broadcast protocol  June 20, 2015August 28, 2016 Guy6 Comments    Background ZooKeeper support clients reading and updating key values pairs with high availability. High availability is achieved by replicating the data to multiple nodes and let clients read from any node. Critical to the design of Zookeeper is the observation that each state change is incremental with respect to the previous state, so there is an implicit dependence on the order of the state changes. Zookeeper Atomic Broadcast (ZAB) is the protocol under the hood that drives ZooKeeper replication order guarantee. It also handles electing a leader and recovery of failing leaders and nodes. This post is about ZAB. Definitions  leader and followers-  in ZooKeeper cluster, one of the nodes has a  leader role and the rest have followers roles. The leader is responsible for accepting all incoming state changes from the clients and replicate them to itself and to the followers. read requests are load balanced between all followers and leader. transactions –  client state changes that a leader propagates to its followers. ‘e’ – the epoch of a leader. epoch is an integer that is generated by a leader when he start to lead and should be larger than epoch’s of previous leaders. ‘c’ – a sequence number that is generated by the leader, starting at 0 and increasing. it is used together with an epoch to order the incoming clients state changes. ‘F.history’ – follower’s history queue. used for committing incoming transactions in the order they arrived. outstanding transactions – the set of transactions in the F.History that have sequence number smaller than current COMMIT sequence number.  ZAB Requirements  Replication guarantees  Reliable delivery – If a transaction, M, is committed by one server, it will be eventually committed by all servers. Total order – If transaction A is committed before transaction B by one server, A will be committed before B by all servers. If A and B are committed messages, either A will be committed before B or B will be committed before A. Causal order – If a transaction B is sent after a transaction A has been committed by the sender of B, A must be ordered before B. If a sender sends C after sending B, C must be ordered after B.   Transactions are replicated as long as majority (quorum) of nodes are up. When nodes fail and later restarted – it should catch up the transactions that were replicated during the time it was down.  ZAB Implementation  clients read from any of the ZooKeeper nodes. clients write state changes to any of the ZooKeeper nodes and this state changes are forward to the leader node. ZooKeeper uses a variation of  two-phase-commit protocol for replicating transactions to followers. When a leader receive a change update from a client it generate a transaction with sequel number c and the leader’s epoch e (see definitions) and send the transaction to all followers. a follower adds the transaction to its history queue and send ACK to the leader. When a leader receives ACK’s from a quorum it send the the quorum COMMIT for that transaction. a follower that accept COMMIT will commit this transaction unless c is higher than any sequence number in its history queue. It will wait for receiving COMMIT’s for all its earlier transactions (outstanding transactions) before commiting.   picture taken from reference [4]  Upon leader crashes, nodes execute a recovery protocol both to agree upon a common consistent state before resuming regular operation and to establish a new leader to broadcast state changes. To exercise the leader role, a node must have the support of a quorum of nodes. As nodes can crash and recover, there can be over time multiple leaders and in fact the same nodes may exercise the node role multiple times. node’s life cycle: each node executes one iteration of this protocol at a time, and at any time, a process may drop the current iteration and start a new one by proceeding to Phase 0.  Phase 0 –  prospective leader election Phase 1 – discovery Phase 2 – synchronization Phase 3 – broadcast   Phases 1 and 2 are important for bringing the ensemble to a mutually consistent state, specially when recovering from crashes. Phase 1 – Discovery In this phase, followers communicate with their prospective leader, so that the leader gathers information about the most recent transactions that its followers accepted. The purpose of this phase is to discover the most updated sequence of accepted transactions among a quorum, and to establish a new epoch so that previous leaders cannot commit new proposals. Because quorum of the followers have all changes accepted by the previous leader- then it is promised that at least one of the followers in current quorum has in its history queue all the changes accepted by previous leader which means that the new leader will have them as well. Phase 1 exact algorithm available here. Phase 2 – Synchronization The Synchronization phase concludes the recovery part of the protocol, synchronizing the replicas in the cluster using the leader’s updated history from the discovery phase. The leader communicates with the followers, proposing transactions from its history. Followers acknowledge the proposals if their own history is behind the leader’s history. When the leader sees acknowledgements from a quorum, it issues a commit message to them. At that point, the leader is said to be established, and not anymore prospective. Phase 2 exact algorithm available here. Phase 3 – Broadcast If no crashes occur, peers stay in this phase indefinitely, performing broadcast of transactions as soon as a ZooKeeper client issues a write request.  Phase 3 exact algorithm available here. To detect failures, Zab employs periodic heartbeat messages between followers and their leaders. If a leader does not receive heartbeats from a quorum of followers within a given timeout, it abandons its leadership and shifts to state election and Phase 0. A follower also goes to Leader Election Phase if it does not receive heartbeats from its leader within a timeout.  References:  Flavio P. Junqueira, Benjamin C. Reed, and Marco Serafini, “Zab: High-performance broadcast for primary-backup systems”  Andr´e Medeiros, “ZooKeeper’s atomic broadcast protocol: Theory and practice” ZooKeeper Apache documentation Benjamin Reed,Flavio P. Junqueira, “A simple totally ordered broadcast protocol”     Advertisements       Share this:TwitterFacebookGoogleLike this:Like Loading...  Related     architecture, atomic, atomic_broadcast, brodcast, commit, crash, protocol, zab, Zookeeper     Post navigation   Previous PostAlgorithm for Zab Phase 3 – BroadcastNext PostRSYNC design – a method for replicating updates      6 thoughts on “Architecture of ZAB – ZooKeeper Atomic Broadcast protocol”       Algorithm for Zab Phase 1 – Discovery | Distributed Algorithm says:     September 28, 2015 at 8:56 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Algorithm for Zab Phase 2 – Synchronization | Distributed Algorithm says:     September 28, 2015 at 8:57 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Algorithm for Zab Phase 3 – Broadcast | Distributed Algorithm says:     September 28, 2015 at 9:03 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Distributed Database Design – Notes | Tingri says:     April 16, 2016 at 9:26 pm      […] ZAB(Zookeeper Atomic Broadcast) seems to be essentially a variant of two phase commit protocol. Attached is a picture. A link explaining it further is here. […]  Reply       Decentralized Web Summit | snarfed.org says:     June 12, 2016 at 6:32 am      […] bitcoin, or send one, and those transactions are just as consistent and durable as a Paxos round or ZAB […]  Reply        www.google.co.uk says:     December 27, 2016 at 7:02 pm      Hey I am so excited I found your website, I really found you by mistake, while I was looking on Bing for something else, Regardless I am here now and would just like to say cheers for a remarkable post and a all round exciting blog (I also love the theme/design), I don’t have time to read through it all at the moment but I have bookmarked it and also added in your RSS feeds, so when I have time I will be back to read a great deal more, Please do keep up the superb job.  Reply     Leave a Reply Cancel reply     Enter your comment here...     Fill in your details below or click an icon to log in:                                    Email (required) (Address never made public)    Name (required)    Website                You are commenting using your WordPress.com account. ( Log Out / Change )              You are commenting using your Twitter account. ( Log Out / Change )              You are commenting using your Facebook account. ( Log Out / Change )              You are commenting using your Google+ account. ( Log Out / Change )     Cancel Connecting to %s      Notify me of new comments via email.              Uncategorized  Architecture of ZAB – ZooKeeper Atomic Broadcast protocol  June 20, 2015August 28, 2016 Guy6 Comments    Background ZooKeeper support clients reading and updating key values pairs with high availability. High availability is achieved by replicating the data to multiple nodes and let clients read from any node. Critical to the design of Zookeeper is the observation that each state change is incremental with respect to the previous state, so there is an implicit dependence on the order of the state changes. Zookeeper Atomic Broadcast (ZAB) is the protocol under the hood that drives ZooKeeper replication order guarantee. It also handles electing a leader and recovery of failing leaders and nodes. This post is about ZAB. Definitions  leader and followers-  in ZooKeeper cluster, one of the nodes has a  leader role and the rest have followers roles. The leader is responsible for accepting all incoming state changes from the clients and replicate them to itself and to the followers. read requests are load balanced between all followers and leader. transactions –  client state changes that a leader propagates to its followers. ‘e’ – the epoch of a leader. epoch is an integer that is generated by a leader when he start to lead and should be larger than epoch’s of previous leaders. ‘c’ – a sequence number that is generated by the leader, starting at 0 and increasing. it is used together with an epoch to order the incoming clients state changes. ‘F.history’ – follower’s history queue. used for committing incoming transactions in the order they arrived. outstanding transactions – the set of transactions in the F.History that have sequence number smaller than current COMMIT sequence number.  ZAB Requirements  Replication guarantees  Reliable delivery – If a transaction, M, is committed by one server, it will be eventually committed by all servers. Total order – If transaction A is committed before transaction B by one server, A will be committed before B by all servers. If A and B are committed messages, either A will be committed before B or B will be committed before A. Causal order – If a transaction B is sent after a transaction A has been committed by the sender of B, A must be ordered before B. If a sender sends C after sending B, C must be ordered after B.   Transactions are replicated as long as majority (quorum) of nodes are up. When nodes fail and later restarted – it should catch up the transactions that were replicated during the time it was down.  ZAB Implementation  clients read from any of the ZooKeeper nodes. clients write state changes to any of the ZooKeeper nodes and this state changes are forward to the leader node. ZooKeeper uses a variation of  two-phase-commit protocol for replicating transactions to followers. When a leader receive a change update from a client it generate a transaction with sequel number c and the leader’s epoch e (see definitions) and send the transaction to all followers. a follower adds the transaction to its history queue and send ACK to the leader. When a leader receives ACK’s from a quorum it send the the quorum COMMIT for that transaction. a follower that accept COMMIT will commit this transaction unless c is higher than any sequence number in its history queue. It will wait for receiving COMMIT’s for all its earlier transactions (outstanding transactions) before commiting.   picture taken from reference [4]  Upon leader crashes, nodes execute a recovery protocol both to agree upon a common consistent state before resuming regular operation and to establish a new leader to broadcast state changes. To exercise the leader role, a node must have the support of a quorum of nodes. As nodes can crash and recover, there can be over time multiple leaders and in fact the same nodes may exercise the node role multiple times. node’s life cycle: each node executes one iteration of this protocol at a time, and at any time, a process may drop the current iteration and start a new one by proceeding to Phase 0.  Phase 0 –  prospective leader election Phase 1 – discovery Phase 2 – synchronization Phase 3 – broadcast   Phases 1 and 2 are important for bringing the ensemble to a mutually consistent state, specially when recovering from crashes. Phase 1 – Discovery In this phase, followers communicate with their prospective leader, so that the leader gathers information about the most recent transactions that its followers accepted. The purpose of this phase is to discover the most updated sequence of accepted transactions among a quorum, and to establish a new epoch so that previous leaders cannot commit new proposals. Because quorum of the followers have all changes accepted by the previous leader- then it is promised that at least one of the followers in current quorum has in its history queue all the changes accepted by previous leader which means that the new leader will have them as well. Phase 1 exact algorithm available here. Phase 2 – Synchronization The Synchronization phase concludes the recovery part of the protocol, synchronizing the replicas in the cluster using the leader’s updated history from the discovery phase. The leader communicates with the followers, proposing transactions from its history. Followers acknowledge the proposals if their own history is behind the leader’s history. When the leader sees acknowledgements from a quorum, it issues a commit message to them. At that point, the leader is said to be established, and not anymore prospective. Phase 2 exact algorithm available here. Phase 3 – Broadcast If no crashes occur, peers stay in this phase indefinitely, performing broadcast of transactions as soon as a ZooKeeper client issues a write request.  Phase 3 exact algorithm available here. To detect failures, Zab employs periodic heartbeat messages between followers and their leaders. If a leader does not receive heartbeats from a quorum of followers within a given timeout, it abandons its leadership and shifts to state election and Phase 0. A follower also goes to Leader Election Phase if it does not receive heartbeats from its leader within a timeout.  References:  Flavio P. Junqueira, Benjamin C. Reed, and Marco Serafini, “Zab: High-performance broadcast for primary-backup systems”  Andr´e Medeiros, “ZooKeeper’s atomic broadcast protocol: Theory and practice” ZooKeeper Apache documentation Benjamin Reed,Flavio P. Junqueira, “A simple totally ordered broadcast protocol”     Advertisements       Share this:TwitterFacebookGoogleLike this:Like Loading...  Related     architecture, atomic, atomic_broadcast, brodcast, commit, crash, protocol, zab, Zookeeper     Uncategorized  Architecture of ZAB – ZooKeeper Atomic Broadcast protocol  June 20, 2015August 28, 2016 Guy6 Comments   Uncategorized UncategorizedUncategorizedArchitecture of ZAB – ZooKeeper Atomic Broadcast protocol June 20, 2015August 28, 2016 Guy6 Comments June 20, 2015August 28, 2016June 20, 2015August 28, 2016June 20, 2015August 28, 2016 GuyGuyGuy6 Comments6 Comments Background ZooKeeper support clients reading and updating key values pairs with high availability. High availability is achieved by replicating the data to multiple nodes and let clients read from any node. Critical to the design of Zookeeper is the observation that each state change is incremental with respect to the previous state, so there is an implicit dependence on the order of the state changes. Zookeeper Atomic Broadcast (ZAB) is the protocol under the hood that drives ZooKeeper replication order guarantee. It also handles electing a leader and recovery of failing leaders and nodes. This post is about ZAB. Definitions  leader and followers-  in ZooKeeper cluster, one of the nodes has a  leader role and the rest have followers roles. The leader is responsible for accepting all incoming state changes from the clients and replicate them to itself and to the followers. read requests are load balanced between all followers and leader. transactions –  client state changes that a leader propagates to its followers. ‘e’ – the epoch of a leader. epoch is an integer that is generated by a leader when he start to lead and should be larger than epoch’s of previous leaders. ‘c’ – a sequence number that is generated by the leader, starting at 0 and increasing. it is used together with an epoch to order the incoming clients state changes. ‘F.history’ – follower’s history queue. used for committing incoming transactions in the order they arrived. outstanding transactions – the set of transactions in the F.History that have sequence number smaller than current COMMIT sequence number.  ZAB Requirements  Replication guarantees  Reliable delivery – If a transaction, M, is committed by one server, it will be eventually committed by all servers. Total order – If transaction A is committed before transaction B by one server, A will be committed before B by all servers. If A and B are committed messages, either A will be committed before B or B will be committed before A. Causal order – If a transaction B is sent after a transaction A has been committed by the sender of B, A must be ordered before B. If a sender sends C after sending B, C must be ordered after B.   Transactions are replicated as long as majority (quorum) of nodes are up. When nodes fail and later restarted – it should catch up the transactions that were replicated during the time it was down.  ZAB Implementation  clients read from any of the ZooKeeper nodes. clients write state changes to any of the ZooKeeper nodes and this state changes are forward to the leader node. ZooKeeper uses a variation of  two-phase-commit protocol for replicating transactions to followers. When a leader receive a change update from a client it generate a transaction with sequel number c and the leader’s epoch e (see definitions) and send the transaction to all followers. a follower adds the transaction to its history queue and send ACK to the leader. When a leader receives ACK’s from a quorum it send the the quorum COMMIT for that transaction. a follower that accept COMMIT will commit this transaction unless c is higher than any sequence number in its history queue. It will wait for receiving COMMIT’s for all its earlier transactions (outstanding transactions) before commiting.   picture taken from reference [4]  Upon leader crashes, nodes execute a recovery protocol both to agree upon a common consistent state before resuming regular operation and to establish a new leader to broadcast state changes. To exercise the leader role, a node must have the support of a quorum of nodes. As nodes can crash and recover, there can be over time multiple leaders and in fact the same nodes may exercise the node role multiple times. node’s life cycle: each node executes one iteration of this protocol at a time, and at any time, a process may drop the current iteration and start a new one by proceeding to Phase 0.  Phase 0 –  prospective leader election Phase 1 – discovery Phase 2 – synchronization Phase 3 – broadcast   Phases 1 and 2 are important for bringing the ensemble to a mutually consistent state, specially when recovering from crashes. Phase 1 – Discovery In this phase, followers communicate with their prospective leader, so that the leader gathers information about the most recent transactions that its followers accepted. The purpose of this phase is to discover the most updated sequence of accepted transactions among a quorum, and to establish a new epoch so that previous leaders cannot commit new proposals. Because quorum of the followers have all changes accepted by the previous leader- then it is promised that at least one of the followers in current quorum has in its history queue all the changes accepted by previous leader which means that the new leader will have them as well. Phase 1 exact algorithm available here. Phase 2 – Synchronization The Synchronization phase concludes the recovery part of the protocol, synchronizing the replicas in the cluster using the leader’s updated history from the discovery phase. The leader communicates with the followers, proposing transactions from its history. Followers acknowledge the proposals if their own history is behind the leader’s history. When the leader sees acknowledgements from a quorum, it issues a commit message to them. At that point, the leader is said to be established, and not anymore prospective. Phase 2 exact algorithm available here. Phase 3 – Broadcast If no crashes occur, peers stay in this phase indefinitely, performing broadcast of transactions as soon as a ZooKeeper client issues a write request.  Phase 3 exact algorithm available here. To detect failures, Zab employs periodic heartbeat messages between followers and their leaders. If a leader does not receive heartbeats from a quorum of followers within a given timeout, it abandons its leadership and shifts to state election and Phase 0. A follower also goes to Leader Election Phase if it does not receive heartbeats from its leader within a timeout.  References:  Flavio P. Junqueira, Benjamin C. Reed, and Marco Serafini, “Zab: High-performance broadcast for primary-backup systems”  Andr´e Medeiros, “ZooKeeper’s atomic broadcast protocol: Theory and practice” ZooKeeper Apache documentation Benjamin Reed,Flavio P. Junqueira, “A simple totally ordered broadcast protocol”     Advertisements       Share this:TwitterFacebookGoogleLike this:Like Loading...  Related  BackgroundBackgroundZooKeeper support clients reading and updating key values pairs with high availability. High availability is achieved by replicating the data to multiple nodes and let clients read from any node. Critical to the design of Zookeeper is the observation that each state change is incremental with respect to the previous state, so there is an implicit dependence on the order of the state changes. Zookeeper Atomic Broadcast (ZAB) is the protocol under the hood that drives ZooKeeper replication order guarantee. It also handles electing a leader and recovery of failing leaders and nodes. This post is about ZAB.DefinitionsDefinitions leader and followers-  in ZooKeeper cluster, one of the nodes has a  leader role and the rest have followers roles. The leader is responsible for accepting all incoming state changes from the clients and replicate them to itself and to the followers. read requests are load balanced between all followers and leader. transactions –  client state changes that a leader propagates to its followers. ‘e’ – the epoch of a leader. epoch is an integer that is generated by a leader when he start to lead and should be larger than epoch’s of previous leaders. ‘c’ – a sequence number that is generated by the leader, starting at 0 and increasing. it is used together with an epoch to order the incoming clients state changes. ‘F.history’ – follower’s history queue. used for committing incoming transactions in the order they arrived. outstanding transactions – the set of transactions in the F.History that have sequence number smaller than current COMMIT sequence number. leader and followers-  in ZooKeeper cluster, one of the nodes has a  leader role and the rest have followers roles. The leader is responsible for accepting all incoming state changes from the clients and replicate them to itself and to the followers. read requests are load balanced between all followers and leader.transactions –  client state changes that a leader propagates to its followers.‘e’ – the epoch of a leader. epoch is an integer that is generated by a leader when he start to lead and should be larger than epoch’s of previous leaders.‘c’ – a sequence number that is generated by the leader, starting at 0 and increasing. it is used together with an epoch to order the incoming clients state changes.‘F.history’ – follower’s history queue. used for committing incoming transactions in the order they arrived.outstanding transactions – the set of transactions in the F.History that have sequence number smaller than current COMMIT sequence number.ZAB RequirementsZAB Requirements Replication guarantees  Reliable delivery – If a transaction, M, is committed by one server, it will be eventually committed by all servers. Total order – If transaction A is committed before transaction B by one server, A will be committed before B by all servers. If A and B are committed messages, either A will be committed before B or B will be committed before A. Causal order – If a transaction B is sent after a transaction A has been committed by the sender of B, A must be ordered before B. If a sender sends C after sending B, C must be ordered after B.   Transactions are replicated as long as majority (quorum) of nodes are up. When nodes fail and later restarted – it should catch up the transactions that were replicated during the time it was down. Replication guarantees  Reliable delivery – If a transaction, M, is committed by one server, it will be eventually committed by all servers. Total order – If transaction A is committed before transaction B by one server, A will be committed before B by all servers. If A and B are committed messages, either A will be committed before B or B will be committed before A. Causal order – If a transaction B is sent after a transaction A has been committed by the sender of B, A must be ordered before B. If a sender sends C after sending B, C must be ordered after B.   Reliable delivery – If a transaction, M, is committed by one server, it will be eventually committed by all servers. Total order – If transaction A is committed before transaction B by one server, A will be committed before B by all servers. If A and B are committed messages, either A will be committed before B or B will be committed before A. Causal order – If a transaction B is sent after a transaction A has been committed by the sender of B, A must be ordered before B. If a sender sends C after sending B, C must be ordered after B. Reliable delivery – If a transaction, M, is committed by one server, it will be eventually committed by all servers.Reliable delivery – Total order – If transaction A is committed before transaction B by one server, A will be committed before B by all servers. If A and B are committed messages, either A will be committed before B or B will be committed before A.Total order – Causal order – If a transaction B is sent after a transaction A has been committed by the sender of B, A must be ordered before B. If a sender sends C after sending B, C must be ordered after B.Causal order – Transactions are replicated as long as majority (quorum) of nodes are up.When nodes fail and later restarted – it should catch up the transactions that were replicated during the time it was down.ZAB ImplementationZAB Implementation clients read from any of the ZooKeeper nodes. clients write state changes to any of the ZooKeeper nodes and this state changes are forward to the leader node. ZooKeeper uses a variation of  two-phase-commit protocol for replicating transactions to followers. When a leader receive a change update from a client it generate a transaction with sequel number c and the leader’s epoch e (see definitions) and send the transaction to all followers. a follower adds the transaction to its history queue and send ACK to the leader. When a leader receives ACK’s from a quorum it send the the quorum COMMIT for that transaction. a follower that accept COMMIT will commit this transaction unless c is higher than any sequence number in its history queue. It will wait for receiving COMMIT’s for all its earlier transactions (outstanding transactions) before commiting. clients read from any of the ZooKeeper nodes.clients write state changes to any of the ZooKeeper nodes and this state changes are forward to the leader node.ZooKeeper uses a variation of  two-phase-commit protocol for replicating transactions to followers. When a leader receive a change update from a client it generate a transaction with sequel number c and the leader’s epoch e (see definitions) and send the transaction to all followers. a follower adds the transaction to its history queue and send ACK to the leader. When a leader receives ACK’s from a quorum it send the the quorum COMMIT for that transaction. a follower that accept COMMIT will commit this transaction unless c is higher than any sequence number in its history queue. It will wait for receiving COMMIT’s for all its earlier transactions (outstanding transactions) before commiting.two-phase-commit protocolcecpicture taken from reference [4] Upon leader crashes, nodes execute a recovery protocol both to agree upon a common consistent state before resuming regular operation and to establish a new leader to broadcast state changes. To exercise the leader role, a node must have the support of a quorum of nodes. As nodes can crash and recover, there can be over time multiple leaders and in fact the same nodes may exercise the node role multiple times. node’s life cycle: each node executes one iteration of this protocol at a time, and at any time, a process may drop the current iteration and start a new one by proceeding to Phase 0.  Phase 0 –  prospective leader election Phase 1 – discovery Phase 2 – synchronization Phase 3 – broadcast   Phases 1 and 2 are important for bringing the ensemble to a mutually consistent state, specially when recovering from crashes. Phase 1 – Discovery In this phase, followers communicate with their prospective leader, so that the leader gathers information about the most recent transactions that its followers accepted. The purpose of this phase is to discover the most updated sequence of accepted transactions among a quorum, and to establish a new epoch so that previous leaders cannot commit new proposals. Because quorum of the followers have all changes accepted by the previous leader- then it is promised that at least one of the followers in current quorum has in its history queue all the changes accepted by previous leader which means that the new leader will have them as well. Phase 1 exact algorithm available here. Phase 2 – Synchronization The Synchronization phase concludes the recovery part of the protocol, synchronizing the replicas in the cluster using the leader’s updated history from the discovery phase. The leader communicates with the followers, proposing transactions from its history. Followers acknowledge the proposals if their own history is behind the leader’s history. When the leader sees acknowledgements from a quorum, it issues a commit message to them. At that point, the leader is said to be established, and not anymore prospective. Phase 2 exact algorithm available here. Phase 3 – Broadcast If no crashes occur, peers stay in this phase indefinitely, performing broadcast of transactions as soon as a ZooKeeper client issues a write request.  Phase 3 exact algorithm available here. To detect failures, Zab employs periodic heartbeat messages between followers and their leaders. If a leader does not receive heartbeats from a quorum of followers within a given timeout, it abandons its leadership and shifts to state election and Phase 0. A follower also goes to Leader Election Phase if it does not receive heartbeats from its leader within a timeout. Upon leader crashes, nodes execute a recovery protocol both to agree upon a common consistent state before resuming regular operation and to establish a new leader to broadcast state changes.To exercise the leader role, a node must have the support of a quorum of nodes. As nodes can crash and recover, there can be over time multiple leaders and in fact the same nodes may exercise the node role multiple times.node’s life cycle: each node executes one iteration of this protocol at a time, and at any time, a process may drop the current iteration and start a new one by proceeding to Phase 0.  Phase 0 –  prospective leader election Phase 1 – discovery Phase 2 – synchronization Phase 3 – broadcast   Phase 0 –  prospective leader election Phase 1 – discovery Phase 2 – synchronization Phase 3 – broadcast Phase 0 –  prospective leader electionPhase 0 –  prospective leader electionPhase 0 –  prospective leader electionPhase 1 – discoveryPhase 1 – Phase 1 – Phase 1 – Phase 1 – discoverydiscoveryPhase 2 – synchronizationPhase 2 – synchronizationPhase 2 – synchronizationPhase Phase Phase 3 – broadcastPhase 3 – broadcastPhase 3 – broadcastPhase Phase Phases 1 and 2 are important for bringing the ensemble to a mutually consistent state, specially when recovering from crashes.Phase 1 – Discovery In this phase, followers communicate with their prospective leader, so that the leader gathers information about the most recent transactions that its followers accepted. The purpose of this phase is to discover the most updated sequence of accepted transactions among a quorum, and to establish a new epoch so that previous leaders cannot commit new proposals. Because quorum of the followers have all changes accepted by the previous leader- then it is promised that at least one of the followers in current quorum has in its history queue all the changes accepted by previous leader which means that the new leader will have them as well. Phase 1 exact algorithm available here.Phase 1 – DiscoveryPhase 1 exact algorithm available here. Phase 2 – Synchronization The Synchronization phase concludes the recovery part of the protocol, synchronizing the replicas in the cluster using the leader’s updated history from the discovery phase. The leader communicates with the followers, proposing transactions from its history. Followers acknowledge the proposals if their own history is behind the leader’s history. When the leader sees acknowledgements from a quorum, it issues a commit message to them. At that point, the leader is said to be established, and not anymore prospective. Phase 2 exact algorithm available here.Phase 2 – Synchronization Phase 2 exact algorithm available here. Phase 3 – Broadcast If no crashes occur, peers stay in this phase indefinitely, performing broadcast of transactions as soon as a ZooKeeper client issues a write request.  Phase 3 exact algorithm available here.Phase 3 – Broadcast Phase 3 exact algorithm available here. To detect failures, Zab employs periodic heartbeat messages between followers and their leaders. If a leader does not receive heartbeats from a quorum of followers within a given timeout, it abandons its leadership and shifts to state election and Phase 0. A follower also goes to Leader Election Phase if it does not receive heartbeats from its leader within a timeout.References:References: Flavio P. Junqueira, Benjamin C. Reed, and Marco Serafini, “Zab: High-performance broadcast for primary-backup systems”  Andr´e Medeiros, “ZooKeeper’s atomic broadcast protocol: Theory and practice” ZooKeeper Apache documentation Benjamin Reed,Flavio P. Junqueira, “A simple totally ordered broadcast protocol” Flavio P. Junqueira, Benjamin C. Reed, and Marco Serafini, “Zab: High-performance broadcast for primary-backup systems” “Zab: High-performance broadcast for primary-backup systems” “Zab: High-performance broadcast for primary-backup systems”Andr´e Medeiros, “ZooKeeper’s atomic broadcast protocol: Theory and practice”“ZooKeeper’s atomic broadcast protocol: Theory and practice”“ZooKeeper’s atomic broadcast protocol: Theory and practice”ZooKeeper Apache documentationZooKeeper Apache documentationZooKeeper Apache documentationBenjamin Reed,Flavio P. Junqueira, “A simple totally ordered broadcast protocol”“A simple totally ordered broadcast protocol”A simple totally ordered broadcast protocol”  Advertisements        Advertisements      Advertisements  Share this:TwitterFacebookGoogleLike this:Like Loading...  Related Share this:TwitterFacebookGoogleShare this:TwitterFacebookGoogleShare this:TwitterFacebookGoogleTwitterFacebookGoogleTwitterTwitterTwitterFacebookFacebookFacebookGoogleGoogleGoogleLike this:Like Loading...Like this:Like Loading...LikeLikeLoading... Related RelatedRelated  architecture, atomic, atomic_broadcast, brodcast, commit, crash, protocol, zab, Zookeeper   architecture, atomic, atomic_broadcast, brodcast, commit, crash, protocol, zab, Zookeeper architecture, atomic, atomic_broadcast, brodcast, commit, crash, protocol, zab, Zookeeperarchitectureatomicatomic_broadcastbrodcastcommitcrashprotocolzabZookeeper Post navigation   Previous PostAlgorithm for Zab Phase 3 – BroadcastNext PostRSYNC design – a method for replicating updates   Post navigation  Previous PostAlgorithm for Zab Phase 3 – BroadcastNext PostRSYNC design – a method for replicating updates   Previous PostAlgorithm for Zab Phase 3 – Broadcast Previous PostAlgorithm for Zab Phase 3 – BroadcastPrevious PostAlgorithm for Zab Phase 3 – BroadcastPrevious PostAlgorithm for Zab Phase 3 – BroadcastNext PostRSYNC design – a method for replicating updates Next PostRSYNC design – a method for replicating updates Next PostRSYNC design – a method for replicating updatesNext PostRSYNC design – a method for replicating updates  6 thoughts on “Architecture of ZAB – ZooKeeper Atomic Broadcast protocol”       Algorithm for Zab Phase 1 – Discovery | Distributed Algorithm says:     September 28, 2015 at 8:56 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Algorithm for Zab Phase 2 – Synchronization | Distributed Algorithm says:     September 28, 2015 at 8:57 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Algorithm for Zab Phase 3 – Broadcast | Distributed Algorithm says:     September 28, 2015 at 9:03 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Distributed Database Design – Notes | Tingri says:     April 16, 2016 at 9:26 pm      […] ZAB(Zookeeper Atomic Broadcast) seems to be essentially a variant of two phase commit protocol. Attached is a picture. A link explaining it further is here. […]  Reply       Decentralized Web Summit | snarfed.org says:     June 12, 2016 at 6:32 am      […] bitcoin, or send one, and those transactions are just as consistent and durable as a Paxos round or ZAB […]  Reply        www.google.co.uk says:     December 27, 2016 at 7:02 pm      Hey I am so excited I found your website, I really found you by mistake, while I was looking on Bing for something else, Regardless I am here now and would just like to say cheers for a remarkable post and a all round exciting blog (I also love the theme/design), I don’t have time to read through it all at the moment but I have bookmarked it and also added in your RSS feeds, so when I have time I will be back to read a great deal more, Please do keep up the superb job.  Reply     Leave a Reply Cancel reply     Enter your comment here...     Fill in your details below or click an icon to log in:                                    Email (required) (Address never made public)    Name (required)    Website                You are commenting using your WordPress.com account. ( Log Out / Change )              You are commenting using your Twitter account. ( Log Out / Change )              You are commenting using your Facebook account. ( Log Out / Change )              You are commenting using your Google+ account. ( Log Out / Change )     Cancel Connecting to %s      Notify me of new comments via email.           6 thoughts on “Architecture of ZAB – ZooKeeper Atomic Broadcast protocol” Architecture of ZAB – ZooKeeper Atomic Broadcast protocol     Algorithm for Zab Phase 1 – Discovery | Distributed Algorithm says:     September 28, 2015 at 8:56 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Algorithm for Zab Phase 2 – Synchronization | Distributed Algorithm says:     September 28, 2015 at 8:57 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Algorithm for Zab Phase 3 – Broadcast | Distributed Algorithm says:     September 28, 2015 at 9:03 am      […] This is part of my main post on architecture of ZAB. […]  Reply       Distributed Database Design – Notes | Tingri says:     April 16, 2016 at 9:26 pm      […] ZAB(Zookeeper Atomic Broadcast) seems to be essentially a variant of two phase commit protocol. Attached is a picture. A link explaining it further is here. […]  Reply       Decentralized Web Summit | snarfed.org says:     June 12, 2016 at 6:32 am      […] bitcoin, or send one, and those transactions are just as consistent and durable as a Paxos round or ZAB […]  Reply        www.google.co.uk says:     December 27, 2016 at 7:02 pm      Hey I am so excited I found your website, I really found you by mistake, while I was looking on Bing for something else, Regardless I am here now and would just like to say cheers for a remarkable post and a all round exciting blog (I also love the theme/design), I don’t have time to read through it all at the moment but I have bookmarked it and also added in your RSS feeds, so when I have time I will be back to read a great deal more, Please do keep up the superb job.  Reply       Algorithm for Zab Phase 1 – Discovery | Distributed Algorithm says:     September 28, 2015 at 8:56 am      […] This is part of my main post on architecture of ZAB. […]  Reply     Algorithm for Zab Phase 1 – Discovery | Distributed Algorithm says:     September 28, 2015 at 8:56 am      […] This is part of my main post on architecture of ZAB. […]  Reply   Algorithm for Zab Phase 1 – Discovery | Distributed Algorithm says:     September 28, 2015 at 8:56 am     Algorithm for Zab Phase 1 – Discovery | Distributed Algorithm says: Algorithm for Zab Phase 1 – Discovery | Distributed AlgorithmAlgorithm for Zab Phase 1 – Discovery | Distributed Algorithmsays:   September 28, 2015 at 8:56 am     September 28, 2015 at 8:56 am   September 28, 2015 at 8:56 am  […] This is part of my main post on architecture of ZAB. […] […] This is part of my main post on architecture of ZAB. […]ReplyReplyReply    Algorithm for Zab Phase 2 – Synchronization | Distributed Algorithm says:     September 28, 2015 at 8:57 am      […] This is part of my main post on architecture of ZAB. […]  Reply     Algorithm for Zab Phase 2 – Synchronization | Distributed Algorithm says:     September 28, 2015 at 8:57 am      […] This is part of my main post on architecture of ZAB. […]  Reply   Algorithm for Zab Phase 2 – Synchronization | Distributed Algorithm says:     September 28, 2015 at 8:57 am     Algorithm for Zab Phase 2 – Synchronization | Distributed Algorithm says: Algorithm for Zab Phase 2 – Synchronization | Distributed AlgorithmAlgorithm for Zab Phase 2 – Synchronization | Distributed Algorithmsays:   September 28, 2015 at 8:57 am     September 28, 2015 at 8:57 am   September 28, 2015 at 8:57 am  […] This is part of my main post on architecture of ZAB. […] […] This is part of my main post on architecture of ZAB. […]ReplyReplyReply    Algorithm for Zab Phase 3 – Broadcast | Distributed Algorithm says:     September 28, 2015 at 9:03 am      […] This is part of my main post on architecture of ZAB. […]  Reply     Algorithm for Zab Phase 3 – Broadcast | Distributed Algorithm says:     September 28, 2015 at 9:03 am      […] This is part of my main post on architecture of ZAB. […]  Reply   Algorithm for Zab Phase 3 – Broadcast | Distributed Algorithm says:     September 28, 2015 at 9:03 am     Algorithm for Zab Phase 3 – Broadcast | Distributed Algorithm says: Algorithm for Zab Phase 3 – Broadcast | Distributed AlgorithmAlgorithm for Zab Phase 3 – Broadcast | Distributed Algorithmsays:   September 28, 2015 at 9:03 am     September 28, 2015 at 9:03 am   September 28, 2015 at 9:03 am  […] This is part of my main post on architecture of ZAB. […] […] This is part of my main post on architecture of ZAB. […]ReplyReplyReply    Distributed Database Design – Notes | Tingri says:     April 16, 2016 at 9:26 pm      […] ZAB(Zookeeper Atomic Broadcast) seems to be essentially a variant of two phase commit protocol. Attached is a picture. A link explaining it further is here. […]  Reply     Distributed Database Design – Notes | Tingri says:     April 16, 2016 at 9:26 pm      […] ZAB(Zookeeper Atomic Broadcast) seems to be essentially a variant of two phase commit protocol. Attached is a picture. A link explaining it further is here. […]  Reply   Distributed Database Design – Notes | Tingri says:     April 16, 2016 at 9:26 pm     Distributed Database Design – Notes | Tingri says: Distributed Database Design – Notes | TingriDistributed Database Design – Notes | Tingrisays:   April 16, 2016 at 9:26 pm     April 16, 2016 at 9:26 pm   April 16, 2016 at 9:26 pm  […] ZAB(Zookeeper Atomic Broadcast) seems to be essentially a variant of two phase commit protocol. Attached is a picture. A link explaining it further is here. […] […] ZAB(Zookeeper Atomic Broadcast) seems to be essentially a variant of two phase commit protocol. Attached is a picture. A link explaining it further is here. […]ReplyReplyReply    Decentralized Web Summit | snarfed.org says:     June 12, 2016 at 6:32 am      […] bitcoin, or send one, and those transactions are just as consistent and durable as a Paxos round or ZAB […]  Reply     Decentralized Web Summit | snarfed.org says:     June 12, 2016 at 6:32 am      […] bitcoin, or send one, and those transactions are just as consistent and durable as a Paxos round or ZAB […]  Reply   Decentralized Web Summit | snarfed.org says:     June 12, 2016 at 6:32 am     Decentralized Web Summit | snarfed.org says: Decentralized Web Summit | snarfed.orgDecentralized Web Summit | snarfed.orgsays:   June 12, 2016 at 6:32 am     June 12, 2016 at 6:32 am   June 12, 2016 at 6:32 am  […] bitcoin, or send one, and those transactions are just as consistent and durable as a Paxos round or ZAB […] […] bitcoin, or send one, and those transactions are just as consistent and durable as a Paxos round or ZAB […]ReplyReplyReply     www.google.co.uk says:     December 27, 2016 at 7:02 pm      Hey I am so excited I found your website, I really found you by mistake, while I was looking on Bing for something else, Regardless I am here now and would just like to say cheers for a remarkable post and a all round exciting blog (I also love the theme/design), I don’t have time to read through it all at the moment but I have bookmarked it and also added in your RSS feeds, so when I have time I will be back to read a great deal more, Please do keep up the superb job.  Reply      www.google.co.uk says:     December 27, 2016 at 7:02 pm      Hey I am so excited I found your website, I really found you by mistake, while I was looking on Bing for something else, Regardless I am here now and would just like to say cheers for a remarkable post and a all round exciting blog (I also love the theme/design), I don’t have time to read through it all at the moment but I have bookmarked it and also added in your RSS feeds, so when I have time I will be back to read a great deal more, Please do keep up the superb job.  Reply    www.google.co.uk says:     December 27, 2016 at 7:02 pm      www.google.co.uk says: www.google.co.ukwww.google.co.uksays:   December 27, 2016 at 7:02 pm     December 27, 2016 at 7:02 pm   December 27, 2016 at 7:02 pm  Hey I am so excited I found your website, I really found you by mistake, while I was looking on Bing for something else, Regardless I am here now and would just like to say cheers for a remarkable post and a all round exciting blog (I also love the theme/design), I don’t have time to read through it all at the moment but I have bookmarked it and also added in your RSS feeds, so when I have time I will be back to read a great deal more, Please do keep up the superb job. Hey I am so excited I found your website, I really found you by mistake, while I was looking on Bing for something else, Regardless I am here now and would just like to say cheers for a remarkable post and a all round exciting blog (I also love the theme/design), I don’t have time to read through it all at the moment but I have bookmarked it and also added in your RSS feeds, so when I have time I will be back to read a great deal more, Please do keep up the superb job.ReplyReplyReply Leave a Reply Cancel reply     Enter your comment here...     Fill in your details below or click an icon to log in:                                    Email (required) (Address never made public)    Name (required)    Website                You are commenting using your WordPress.com account. ( Log Out / Change )              You are commenting using your Twitter account. ( Log Out / Change )              You are commenting using your Facebook account. ( Log Out / Change )              You are commenting using your Google+ account. ( Log Out / Change )     Cancel Connecting to %s      Notify me of new comments via email.        Leave a Reply Cancel replyCancel replyCancel reply    Enter your comment here...     Fill in your details below or click an icon to log in:                                    Email (required) (Address never made public)    Name (required)    Website                You are commenting using your WordPress.com account. ( Log Out / Change )              You are commenting using your Twitter account. ( Log Out / Change )              You are commenting using your Facebook account. ( Log Out / Change )              You are commenting using your Google+ account. ( Log Out / Change )     Cancel Connecting to %s      Notify me of new comments via email.        Enter your comment here...  Enter your comment here...  Fill in your details below or click an icon to log in:                                    Email (required) (Address never made public)    Name (required)    Website                You are commenting using your WordPress.com account. ( Log Out / Change )              You are commenting using your Twitter account. ( Log Out / Change )              You are commenting using your Facebook account. ( Log Out / Change )              You are commenting using your Google+ account. ( Log Out / Change )     Cancel Connecting to %s   Fill in your details below or click an icon to log in:                          Fill in your details below or click an icon to log in:                                                           Email (required) (Address never made public)    Name (required)    Website             Email (required) (Address never made public)    Name (required)    Website            Email (required) (Address never made public)    Name (required)    Website    Email (required) (Address never made public)  Email (required) (Address never made public)(required)(Address never made public) Name (required)  Name (required)(required) Website  Website          You are commenting using your WordPress.com account. ( Log Out / Change )            You are commenting using your WordPress.com account. ( Log Out / Change )         You are commenting using your WordPress.com account. ( Log Out / Change )  You are commenting using your WordPress.com account. ( Log Out / Change )( Log Out / Change )Log OutChange          You are commenting using your Twitter account. ( Log Out / Change )            You are commenting using your Twitter account. ( Log Out / Change )         You are commenting using your Twitter account. ( Log Out / Change )  You are commenting using your Twitter account. ( Log Out / Change )( Log Out / Change )Log OutChange          You are commenting using your Facebook account. ( Log Out / Change )            You are commenting using your Facebook account. ( Log Out / Change )         You are commenting using your Facebook account. ( Log Out / Change )  You are commenting using your Facebook account. ( Log Out / Change )( Log Out / Change )Log OutChange          You are commenting using your Google+ account. ( Log Out / Change )            You are commenting using your Google+ account. ( Log Out / Change )         You are commenting using your Google+ account. ( Log Out / Change )  You are commenting using your Google+ account. ( Log Out / Change )( Log Out / Change )Log OutChange Cancel Connecting to %s CancelCancelConnecting to %s  Notify me of new comments via email. Notify me of new comments via email.Notify me of new comments via email.      Search for:      Recent Posts   Performance test for evaluating the overhead of tracing process memory activity   The Architecture of Kafka -Distributed Message Queue   Sketchnote for One line of working code is worth 500 of specification   Sketchnote for Skyscrapers aren’t scalable   Disaster Recovery for Solr – a Story from the Field    Recent Comments  www.google.co.uk on Architecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Disaster Recovery fo… on Two-Phase-CommitDistributed Database… on Architecture of ZAB – Zo…Disaster Recovery fo… on RSYNC design – a method…  Archives  November 2016 August 2016 June 2016 February 2016 January 2016 August 2015 June 2015 May 2015 September 2014 June 2014 May 2014  Categories  file_sharing  Torrent  Uncategorized   Meta  Register Log in Entries RSS Comments RSS WordPress.com    Search for:      Search for:     Search for:  Search for: Recent Posts   Performance test for evaluating the overhead of tracing process memory activity   The Architecture of Kafka -Distributed Message Queue   Sketchnote for One line of working code is worth 500 of specification   Sketchnote for Skyscrapers aren’t scalable   Disaster Recovery for Solr – a Story from the Field   Recent Posts  Performance test for evaluating the overhead of tracing process memory activity   The Architecture of Kafka -Distributed Message Queue   Sketchnote for One line of working code is worth 500 of specification   Sketchnote for Skyscrapers aren’t scalable   Disaster Recovery for Solr – a Story from the Field   Performance test for evaluating the overhead of tracing process memory activity Performance test for evaluating the overhead of tracing process memory activity The Architecture of Kafka -Distributed Message Queue The Architecture of Kafka -Distributed Message Queue Sketchnote for One line of working code is worth 500 of specification Sketchnote for One line of working code is worth 500 of specification Sketchnote for Skyscrapers aren’t scalable Sketchnote for Skyscrapers aren’t scalable Disaster Recovery for Solr – a Story from the Field Disaster Recovery for Solr – a Story from the FieldRecent Comments  www.google.co.uk on Architecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Disaster Recovery fo… on Two-Phase-CommitDistributed Database… on Architecture of ZAB – Zo…Disaster Recovery fo… on RSYNC design – a method…  Recent Comments www.google.co.uk on Architecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Disaster Recovery fo… on Two-Phase-CommitDistributed Database… on Architecture of ZAB – Zo…Disaster Recovery fo… on RSYNC design – a method… www.google.co.uk on Architecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Disaster Recovery fo… on Two-Phase-CommitDistributed Database… on Architecture of ZAB – Zo…Disaster Recovery fo… on RSYNC design – a method… www.google.co.uk on Architecture of ZAB – Zo…www.google.co.uk on Architecture of ZAB – Zo…www.google.co.ukArchitecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Decentralized Web Su…Architecture of ZAB – Zo…Disaster Recovery fo… on Two-Phase-CommitDisaster Recovery fo… on Two-Phase-CommitDisaster Recovery fo…Two-Phase-CommitDistributed Database… on Architecture of ZAB – Zo…Distributed Database… on Architecture of ZAB – Zo…Distributed Database…Architecture of ZAB – Zo…Disaster Recovery fo… on RSYNC design – a method…Disaster Recovery fo… on RSYNC design – a method…Disaster Recovery fo…RSYNC design – a method…Archives  November 2016 August 2016 June 2016 February 2016 January 2016 August 2015 June 2015 May 2015 September 2014 June 2014 May 2014  Archives November 2016 August 2016 June 2016 February 2016 January 2016 August 2015 June 2015 May 2015 September 2014 June 2014 May 2014 November 2016November 2016August 2016August 2016June 2016June 2016February 2016February 2016January 2016January 2016August 2015August 2015June 2015June 2015May 2015May 2015September 2014September 2014June 2014June 2014May 2014May 2014Categories  file_sharing  Torrent  Uncategorized   Categories file_sharing  Torrent  Uncategorized  file_sharing file_sharingTorrent TorrentUncategorized UncategorizedMeta  Register Log in Entries RSS Comments RSS WordPress.com  Meta Register Log in Entries RSS Comments RSS WordPress.com RegisterRegisterLog inLog inEntries RSSEntries RSSRSSComments RSSComments RSSRSSWordPress.comWordPress.com     Search for:      Recent Posts   Performance test for evaluating the overhead of tracing process memory activity   The Architecture of Kafka -Distributed Message Queue   Sketchnote for One line of working code is worth 500 of specification   Sketchnote for Skyscrapers aren’t scalable   Disaster Recovery for Solr – a Story from the Field    Recent Comments  www.google.co.uk on Architecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Disaster Recovery fo… on Two-Phase-CommitDistributed Database… on Architecture of ZAB – Zo…Disaster Recovery fo… on RSYNC design – a method…  Archives  November 2016 August 2016 June 2016 February 2016 January 2016 August 2015 June 2015 May 2015 September 2014 June 2014 May 2014  Categories  file_sharing  Torrent  Uncategorized   Meta  Register Log in Entries RSS Comments RSS WordPress.com      Blog at WordPress.com.      Search for:      Recent Posts   Performance test for evaluating the overhead of tracing process memory activity   The Architecture of Kafka -Distributed Message Queue   Sketchnote for One line of working code is worth 500 of specification   Sketchnote for Skyscrapers aren’t scalable   Disaster Recovery for Solr – a Story from the Field    Recent Comments  www.google.co.uk on Architecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Disaster Recovery fo… on Two-Phase-CommitDistributed Database… on Architecture of ZAB – Zo…Disaster Recovery fo… on RSYNC design – a method…  Archives  November 2016 August 2016 June 2016 February 2016 January 2016 August 2015 June 2015 May 2015 September 2014 June 2014 May 2014  Categories  file_sharing  Torrent  Uncategorized   Meta  Register Log in Entries RSS Comments RSS WordPress.com       Search for:      Recent Posts   Performance test for evaluating the overhead of tracing process memory activity   The Architecture of Kafka -Distributed Message Queue   Sketchnote for One line of working code is worth 500 of specification   Sketchnote for Skyscrapers aren’t scalable   Disaster Recovery for Solr – a Story from the Field    Recent Comments  www.google.co.uk on Architecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Disaster Recovery fo… on Two-Phase-CommitDistributed Database… on Architecture of ZAB – Zo…Disaster Recovery fo… on RSYNC design – a method…  Archives  November 2016 August 2016 June 2016 February 2016 January 2016 August 2015 June 2015 May 2015 September 2014 June 2014 May 2014  Categories  file_sharing  Torrent  Uncategorized   Meta  Register Log in Entries RSS Comments RSS WordPress.com     Search for:      Search for:     Search for:  Search for: Recent Posts   Performance test for evaluating the overhead of tracing process memory activity   The Architecture of Kafka -Distributed Message Queue   Sketchnote for One line of working code is worth 500 of specification   Sketchnote for Skyscrapers aren’t scalable   Disaster Recovery for Solr – a Story from the Field   Recent Posts  Performance test for evaluating the overhead of tracing process memory activity   The Architecture of Kafka -Distributed Message Queue   Sketchnote for One line of working code is worth 500 of specification   Sketchnote for Skyscrapers aren’t scalable   Disaster Recovery for Solr – a Story from the Field   Performance test for evaluating the overhead of tracing process memory activity Performance test for evaluating the overhead of tracing process memory activity The Architecture of Kafka -Distributed Message Queue The Architecture of Kafka -Distributed Message Queue Sketchnote for One line of working code is worth 500 of specification Sketchnote for One line of working code is worth 500 of specification Sketchnote for Skyscrapers aren’t scalable Sketchnote for Skyscrapers aren’t scalable Disaster Recovery for Solr – a Story from the Field Disaster Recovery for Solr – a Story from the FieldRecent Comments  www.google.co.uk on Architecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Disaster Recovery fo… on Two-Phase-CommitDistributed Database… on Architecture of ZAB – Zo…Disaster Recovery fo… on RSYNC design – a method…  Recent Comments www.google.co.uk on Architecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Disaster Recovery fo… on Two-Phase-CommitDistributed Database… on Architecture of ZAB – Zo…Disaster Recovery fo… on RSYNC design – a method… www.google.co.uk on Architecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Disaster Recovery fo… on Two-Phase-CommitDistributed Database… on Architecture of ZAB – Zo…Disaster Recovery fo… on RSYNC design – a method… www.google.co.uk on Architecture of ZAB – Zo…www.google.co.uk on Architecture of ZAB – Zo…www.google.co.ukArchitecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Decentralized Web Su… on Architecture of ZAB – Zo…Decentralized Web Su…Architecture of ZAB – Zo…Disaster Recovery fo… on Two-Phase-CommitDisaster Recovery fo… on Two-Phase-CommitDisaster Recovery fo…Two-Phase-CommitDistributed Database… on Architecture of ZAB – Zo…Distributed Database… on Architecture of ZAB – Zo…Distributed Database…Architecture of ZAB – Zo…Disaster Recovery fo… on RSYNC design – a method…Disaster Recovery fo… on RSYNC design – a method…Disaster Recovery fo…RSYNC design – a method…Archives  November 2016 August 2016 June 2016 February 2016 January 2016 August 2015 June 2015 May 2015 September 2014 June 2014 May 2014  Archives November 2016 August 2016 June 2016 February 2016 January 2016 August 2015 June 2015 May 2015 September 2014 June 2014 May 2014 November 2016November 2016August 2016August 2016June 2016June 2016February 2016February 2016January 2016January 2016August 2015August 2015June 2015June 2015May 2015May 2015September 2014September 2014June 2014June 2014May 2014May 2014Categories  file_sharing  Torrent  Uncategorized   Categories file_sharing  Torrent  Uncategorized  file_sharing file_sharingTorrent TorrentUncategorized UncategorizedMeta  Register Log in Entries RSS Comments RSS WordPress.com  Meta Register Log in Entries RSS Comments RSS WordPress.com RegisterRegisterLog inLog inEntries RSSEntries RSSRSSComments RSSComments RSSRSSWordPress.comWordPress.com Blog at WordPress.com. Blog at WordPress.com.Back to topBack to topBack to top       Post to    Cancel             Post to    Cancel         Post to  Cancel       CancelCancel%d bloggers like this:%d bloggers like this:%d  